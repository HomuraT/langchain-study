# ChatModels æŒ‡å—

ChatModels æ˜¯ LangChain ä¸­ç”¨äºä¸èŠå¤©æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„ GPT ç³»åˆ—ï¼‰è¿›è¡Œäº¤äº’çš„æ ¸å¿ƒç»„ä»¶ã€‚æœ¬æŒ‡å—å°†å…¨é¢ä»‹ç» ChatModels çš„å„ç§åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚

è¯¦ç»†æµ‹è¯•æ ·ä¾‹ä»¥åŠæºç ï¼Œå¯ä»¥è®¿é—®[github](https://github.com/HomuraT/langchain-study/tree/main/unitests/test_chatmodels)

## ğŸ“‹ ç›®å½•

1. [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
2. [åŸºç¡€èŠå¤©åŠŸèƒ½](#åŸºç¡€èŠå¤©åŠŸèƒ½)
3. [å¼‚æ­¥æ“ä½œ](#å¼‚æ­¥æ“ä½œ)
4. [æµå¼è¾“å‡º](#æµå¼è¾“å‡º)
5. [å¤šæ¨¡æ€åŠŸèƒ½](#å¤šæ¨¡æ€åŠŸèƒ½)
6. [å·¥å…·è°ƒç”¨](#å·¥å…·è°ƒç”¨)
7. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
8. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
9. [å“åº”å…ƒæ•°æ®](#å“åº”å…ƒæ•°æ®)
10. [Token ä½¿ç”¨è¿½è¸ª](#token-ä½¿ç”¨è¿½è¸ª)

## åŸºç¡€æ¦‚å¿µ

### ChatModels æ˜¯ä»€ä¹ˆï¼Ÿ

ChatModels æ˜¯ LangChain çš„èŠå¤©æ¨¡å‹æŠ½è±¡ï¼Œæä¾›äº†ä¸å„ç§å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯çš„ç»Ÿä¸€æ¥å£ã€‚å®ƒæ”¯æŒå¤šç§åŠŸèƒ½ï¼Œä»ç®€å•çš„æ–‡æœ¬å¯¹è¯åˆ°å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’å’Œå·¥å…·è°ƒç”¨ã€‚

**ç›¸å…³é“¾æ¥**ï¼š
- [LangChain Chat Models æ¦‚å¿µ](https://python.langchain.com/docs/concepts/chat_models/)
- [å¦‚ä½•ä½¿ç”¨èŠå¤©æ¨¡å‹](https://python.langchain.com/docs/how_to/chat_models_universal_init/)

### æ ¸å¿ƒç»„ä»¶

- **ChatOpenAI**: OpenAI èŠå¤©æ¨¡å‹çš„å®ç°
- **Messages**: å¯¹è¯æ¶ˆæ¯çš„æ•°æ®ç»“æ„
- **Tools**: å¤–éƒ¨åŠŸèƒ½è°ƒç”¨çš„æŠ½è±¡
- **Callbacks**: å›è°ƒå¤„ç†æœºåˆ¶

## åŸºç¡€èŠå¤©åŠŸèƒ½

### æ¨¡å‹åˆå§‹åŒ–

ChatModels çš„ä½¿ç”¨ä»åˆ›å»ºæ¨¡å‹å®ä¾‹å¼€å§‹ï¼š

```python
from langchain_openai import ChatOpenAI

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = ChatOpenAI(
    model="gpt-4o-mini",
    base_url="http://localhost:8212",
    api_key="your-api-key",
    temperature=0.7,
    max_tokens=1000,
    timeout=30
)
```

**å‚æ•°è¯´æ˜**ï¼š
- `model`: ä½¿ç”¨çš„æ¨¡å‹åç§°
- `temperature`: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-2ï¼Œ0æœ€ç¡®å®šï¼Œ2æœ€éšæœºï¼‰
- `max_tokens`: æœ€å¤§è¾“å‡ºtokenæ•°é‡
- `timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

### æ¶ˆæ¯ç±»å‹

LangChain æä¾›äº†å¤šç§æ¶ˆæ¯ç±»å‹æ¥æ„å»ºå¯¹è¯ï¼š

```python
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

# ç³»ç»Ÿæ¶ˆæ¯ - å®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸º
system_msg = SystemMessage(content="You are a helpful assistant.")

# äººç±»æ¶ˆæ¯ - ç”¨æˆ·è¾“å…¥
human_msg = HumanMessage(content="Hello, how are you?")

# AIæ¶ˆæ¯ - AIçš„å›å¤
ai_msg = AIMessage(content="I'm doing well, thank you!")
```

### åŸºç¡€å¯¹è¯

**è¾“å…¥**: æ¶ˆæ¯åˆ—è¡¨
**è¾“å‡º**: AIMessage å¯¹è±¡
**åŸç†**: æ¨¡å‹æ¥æ”¶æ¶ˆæ¯å†å²ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå›å¤

```python
# ç®€å•å¯¹è¯
response = model.invoke([HumanMessage(content="Hello!")])
print(response.content)

# å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„å¯¹è¯
messages = [
    SystemMessage(content="You are a math tutor."),
    HumanMessage(content="What is 2+2?")
]
response = model.invoke(messages)
print(response.content)
```

### å¤šè½®å¯¹è¯

**åŸç†**: é€šè¿‡ç»´æŠ¤æ¶ˆæ¯å†å²æ¥ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡

```python
# æ„å»ºå¯¹è¯å†å²
conversation = [
    HumanMessage(content="My name is Alice."),
    AIMessage(content="Nice to meet you, Alice!"),
    HumanMessage(content="What's my name?")
]

response = model.invoke(conversation)
print(response.content)  # åº”è¯¥åŒ…å« "Alice"
```

### æ‰¹å¤„ç†

**è¾“å…¥**: æ¶ˆæ¯æ‰¹æ¬¡åˆ—è¡¨
**è¾“å‡º**: AIMessage åˆ—è¡¨
**åŸç†**: å¹¶è¡Œå¤„ç†å¤šä¸ªç‹¬ç«‹çš„å¯¹è¯è¯·æ±‚

```python
message_batches = [
    [HumanMessage(content="Hello 1")],
    [HumanMessage(content="Hello 2")],
    [HumanMessage(content="Hello 3")]
]

responses = model.batch(message_batches)
for i, response in enumerate(responses):
    print(f"Response {i+1}: {response.content}")
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•è¿›è¡ŒèŠå¤©å¯¹è¯](https://python.langchain.com/docs/how_to/chatbots/)

## å¼‚æ­¥æ“ä½œ

### æ¦‚å¿µå®šä¹‰

å¼‚æ­¥æ“ä½œå…è®¸éé˜»å¡å¼çš„æ¨¡å‹è°ƒç”¨ï¼Œæé«˜åº”ç”¨ç¨‹åºçš„å¹¶å‘æ€§èƒ½å’Œå“åº”é€Ÿåº¦ã€‚

### å¼‚æ­¥è°ƒç”¨

**è¾“å…¥**: æ¶ˆæ¯åˆ—è¡¨
**è¾“å‡º**: AIMessage å¯¹è±¡ï¼ˆå¼‚æ­¥ï¼‰
**åŸç†**: ä½¿ç”¨ Python asyncio å®ç°éé˜»å¡è°ƒç”¨

```python
import asyncio

async def async_chat():
    response = await model.ainvoke([HumanMessage(content="Hello async!")])
    return response.content

# è¿è¡Œå¼‚æ­¥å‡½æ•°
result = asyncio.run(async_chat())
print(result)
```

### å¼‚æ­¥æµå¼è¾“å‡º

**åŸç†**: ç»“åˆå¼‚æ­¥å’Œæµå¼è¾“å‡ºï¼Œå®ç°éé˜»å¡çš„å®æ—¶å“åº”

```python
async def async_stream_chat():
    async for chunk in model.astream([HumanMessage(content="Count to 5")]):
        print(chunk.content, end="", flush=True)

asyncio.run(async_stream_chat())
```

### å¹¶å‘è¯·æ±‚å¤„ç†

**åŸç†**: åˆ©ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œæ˜¾è‘—æå‡ååé‡

```python
async def concurrent_requests():
    tasks = [
        model.ainvoke([HumanMessage(content=f"Request {i}")])
        for i in range(5)
    ]
    
    responses = await asyncio.gather(*tasks)
    return [r.content for r in responses]

results = asyncio.run(concurrent_requests())
```

### å¼‚æ­¥æ‰¹å¤„ç†

```python
async def async_batch():
    message_batches = [
        [HumanMessage(content=f"Batch {i}")]
        for i in range(3)
    ]
    
    responses = await model.abatch(message_batches)
    return responses

results = asyncio.run(async_batch())
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¼‚æ­¥ç¼–ç¨‹æ¦‚å¿µ](https://python.langchain.com/docs/concepts/async/)

## æµå¼è¾“å‡º

### æ¦‚å¿µå®šä¹‰

æµå¼è¾“å‡ºå…è®¸æ¨¡å‹é€æ­¥ç”Ÿæˆå’Œè¿”å›å“åº”å†…å®¹ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”åä¸€æ¬¡æ€§è¿”å›ï¼Œä»è€Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

### åŸºç¡€æµå¼è¾“å‡º

**è¾“å…¥**: æ¶ˆæ¯åˆ—è¡¨
**è¾“å‡º**: ChatGenerationChunk è¿­ä»£å™¨
**åŸç†**: æ¨¡å‹ç”Ÿæˆå†…å®¹æ—¶å®æ—¶è¿”å›éƒ¨åˆ†ç»“æœ

```python
# å¯ç”¨æµå¼è¾“å‡º
streaming_model = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True
)

# è·å–æµå¼å“åº”
for chunk in streaming_model.stream([HumanMessage(content="Tell me a story")]):
    print(chunk.content, end="", flush=True)
```

### æµå¼å›è°ƒå¤„ç†

**åŸç†**: é€šè¿‡å›è°ƒå‡½æ•°è‡ªå®šä¹‰æµå¼è¾“å‡ºçš„å¤„ç†é€»è¾‘

```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler

class CustomCallbackHandler(StreamingStdOutCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"Token: {token}")

model_with_callback = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True,
    callbacks=[CustomCallbackHandler()]
)
```

### æµå¼ vs æ™®é€šè¾“å‡ºå¯¹æ¯”

```python
# æµå¼è¾“å‡º - å®æ—¶æ˜¾ç¤º
chunks = list(streaming_model.stream([HumanMessage(content="What is AI?")]))
streaming_response = "".join([chunk.content for chunk in chunks])

# æ™®é€šè¾“å‡º - ç­‰å¾…å®Œæ•´å“åº”
normal_response = model.invoke([HumanMessage(content="What is AI?")])

print(f"Streaming: {streaming_response}")
print(f"Normal: {normal_response.content}")
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•æµå¼è¾“å‡ºèŠå¤©æ¨¡å‹å“åº”](https://python.langchain.com/docs/how_to/streaming/)

## å¤šæ¨¡æ€åŠŸèƒ½

### æ¦‚å¿µå®šä¹‰

å¤šæ¨¡æ€åŠŸèƒ½å…è®¸ ChatModels å¤„ç†æ–‡æœ¬ä¹‹å¤–çš„å…¶ä»–æ•°æ®ç±»å‹ï¼Œå¦‚å›¾åƒã€PDFæ–‡æ¡£ã€éŸ³é¢‘ç­‰ï¼Œå®ç°çœŸæ­£çš„å¤šæ¨¡æ€ç†è§£å’Œäº¤äº’ã€‚

### å›¾åƒå¤„ç†

**è¾“å…¥**: åŒ…å«å›¾åƒæ•°æ®çš„æ¶ˆæ¯
**è¾“å‡º**: åŸºäºå›¾åƒå†…å®¹çš„æ–‡æœ¬æè¿°
**åŸç†**: æ¨¡å‹ä½¿ç”¨è§†è§‰ç¼–ç å™¨åˆ†æå›¾åƒå†…å®¹

```python
# å¤„ç† base64 ç¼–ç çš„å›¾åƒ
import base64

with open("image.png", "rb") as f:
    image_data = base64.b64encode(f.read()).decode("utf-8")

message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe this image:"},
        {
            "type": "image",
            "source_type": "base64",
            "data": image_data,
            "mime_type": "image/png"
        }
    ]
}

response = model.invoke([message])
print(response.content)
```

```python
# å¤„ç† URL å›¾åƒ
message = {
    "role": "user", 
    "content": [
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image",
            "source_type": "url", 
            "url": "https://example.com/image.jpg"
        }
    ]
}

response = model.invoke([message])
```

### PDF æ–‡æ¡£å¤„ç†

**åŸç†**: æ¨¡å‹è§£æPDFæ–‡æ¡£ç»“æ„å’Œæ–‡æœ¬å†…å®¹

```python
# å¤„ç† PDF æ–‡æ¡£
with open("document.pdf", "rb") as f:
    pdf_data = base64.b64encode(f.read()).decode("utf-8")

message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Summarize this PDF:"},
        {
            "type": "file",
            "source_type": "base64",
            "data": pdf_data,
            "mime_type": "application/pdf",
            "filename": "document.pdf"
        }
    ]
}

response = model.invoke([message])
```

### å¤šå›¾åƒå¯¹æ¯”

```python
# å¯¹æ¯”å¤šå¼ å›¾åƒ
message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Compare these images:"},
        {"type": "image", "source_type": "url", "url": "image1.jpg"},
        {"type": "image", "source_type": "url", "url": "image2.jpg"}
    ]
}

response = model.invoke([message])
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•ä¼ é€’å¤šæ¨¡æ€æ•°æ®](https://python.langchain.com/docs/how_to/multimodal_inputs/)

## å·¥å…·è°ƒç”¨

### æ¦‚å¿µå®šä¹‰

å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰æ˜¯è®© AI æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å‡½æ•°å’Œ API çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæå¤§æ‰©å±•äº†æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚

### å·¥å…·å®šä¹‰

**åŸç†**: ä½¿ç”¨è£…é¥°å™¨å®šä¹‰å·¥å…·ï¼ŒLangChain è‡ªåŠ¨ç”Ÿæˆå·¥å…·æè¿°ä¾›æ¨¡å‹ä½¿ç”¨

```python
from langchain_core.tools import tool

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers together.
    
    Args:
        a: First number
        b: Second number
        
    Returns:
        Sum of the two numbers
    """
    return a + b

@tool
def get_weather(location: str) -> dict:
    """Get weather information for a location.
    
    Args:
        location: City name
        
    Returns:
        Weather data dictionary
    """
    return {
        "location": location,
        "temperature": 22,
        "description": "sunny"
    }
```

### å·¥å…·ç»‘å®šå’Œè°ƒç”¨

**è¾“å…¥**: ç»‘å®šå·¥å…·çš„æ¨¡å‹å’Œæ¶ˆæ¯
**è¾“å‡º**: åŒ…å«å·¥å…·è°ƒç”¨çš„ AIMessage
**åŸç†**: æ¨¡å‹è¯†åˆ«éœ€è¦ä½¿ç”¨å·¥å…·çš„åœºæ™¯ï¼Œç”Ÿæˆå·¥å…·è°ƒç”¨æŒ‡ä»¤

```python
# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
model_with_tools = model.bind_tools([add_numbers, get_weather])

# è¯·æ±‚è®¡ç®—
response = model_with_tools.invoke([
    HumanMessage(content="What is 15 + 27?")
])

# æ£€æŸ¥å·¥å…·è°ƒç”¨
if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"Tool: {tool_call['name']}")
        print(f"Args: {tool_call['args']}")
```

### å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹

**åŸç†**: å®Œæ•´çš„å·¥å…·è°ƒç”¨åŒ…æ‹¬ï¼šè¯·æ±‚â†’å·¥å…·è°ƒç”¨â†’æ‰§è¡Œâ†’ç»“æœè¿”å›â†’æœ€ç»ˆå›ç­”

```python
from langchain_core.messages import ToolMessage

def complete_tool_calling(query: str):
    # 1. å‘é€æŸ¥è¯¢
    messages = [HumanMessage(content=query)]
    
    # 2. æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·
    ai_response = model_with_tools.invoke(messages)
    messages.append(ai_response)
    
    # 3. æ‰§è¡Œå·¥å…·è°ƒç”¨
    for tool_call in ai_response.tool_calls:
        if tool_call["name"] == "add_numbers":
            result = add_numbers.invoke(tool_call)
        elif tool_call["name"] == "get_weather":
            result = get_weather.invoke(tool_call)
        
        # 4. æ·»åŠ å·¥å…·ç»“æœ
        tool_message = ToolMessage(
            content=str(result.content),
            tool_call_id=tool_call["id"],
            name=tool_call["name"]
        )
        messages.append(tool_message)
    
    # 5. è·å–æœ€ç»ˆå›ç­”
    final_response = model_with_tools.invoke(messages)
    return final_response.content

# ä½¿ç”¨ç¤ºä¾‹
result = complete_tool_calling("Add 15 and 27, then tell me the weather in Beijing")
```

### å¹¶è¡Œå·¥å…·è°ƒç”¨

**åŸç†**: æ¨¡å‹å¯ä»¥åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·æé«˜æ•ˆç‡

```python
response = model_with_tools.invoke([
    HumanMessage(content="Calculate 5+3 and get weather for Tokyo")
])

# å¯èƒ½åŒæ—¶è°ƒç”¨ add_numbers å’Œ get_weather
for tool_call in response.tool_calls:
    print(f"Parallel tool call: {tool_call['name']}")
```

### å¤æ‚å·¥å…·ç¤ºä¾‹

```python
@tool
def search_database(query: str, table: str) -> list:
    """Search database for information.
    
    Args:
        query: Search query
        table: Database table name
        
    Returns:
        List of matching records
    """
    # æ¨¡æ‹Ÿæ•°æ®åº“æœç´¢
    return [{"id": 1, "name": "Result 1"}]

@tool  
def send_email(recipient: str, subject: str, body: str) -> bool:
    """Send an email.
    
    Args:
        recipient: Email recipient
        subject: Email subject
        body: Email body
        
    Returns:
        Success status
    """
    print(f"Email sent to {recipient}")
    return True
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•è¿›è¡Œå·¥å…·/å‡½æ•°è°ƒç”¨](https://python.langchain.com/docs/how_to/tool_calling/)
- [å¦‚ä½•åˆ›å»ºå·¥å…·](https://python.langchain.com/docs/how_to/custom_tools/)

## é«˜çº§åŠŸèƒ½

### ç»“æ„åŒ–è¾“å‡º

**æ¦‚å¿µ**: è®©æ¨¡å‹è¿”å›ç¬¦åˆç‰¹å®šæ•°æ®ç»“æ„çš„æ ¼å¼åŒ–å“åº”
**åŸç†**: ä½¿ç”¨ Pydantic æ¨¡å‹çº¦æŸè¾“å‡ºæ ¼å¼

```python
from pydantic import BaseModel, Field

class WeatherInfo(BaseModel):
    """å¤©æ°”ä¿¡æ¯ç»“æ„"""
    location: str = Field(description="åœ°ç‚¹åç§°")
    temperature: float = Field(description="æ¸©åº¦")
    humidity: int = Field(description="æ¹¿åº¦ç™¾åˆ†æ¯”")
    description: str = Field(description="å¤©æ°”æè¿°")

# åˆ›å»ºç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
structured_model = model.with_structured_output(WeatherInfo)

# è·å–ç»“æ„åŒ–å“åº”
weather = structured_model.invoke([
    HumanMessage(content="Tell me about the weather in Beijing")
])

print(f"Location: {weather.location}")
print(f"Temperature: {weather.temperature}Â°C")
```

### å¤æ‚ç»“æ„åŒ–è¾“å‡º

```python
class Person(BaseModel):
    name: str = Field(description="äººå‘˜å§“å")
    age: int = Field(description="äººå‘˜å¹´é¾„")
    skills: list[str] = Field(description="æŠ€èƒ½åˆ—è¡¨")

class Team(BaseModel):
    team_name: str = Field(description="å›¢é˜Ÿåç§°")
    members: list[Person] = Field(description="å›¢é˜Ÿæˆå‘˜")
    project: str = Field(description="é¡¹ç›®åç§°")

team_model = model.with_structured_output(Team)
result = team_model.invoke([
    HumanMessage(content="Create a 3-person development team for a web project")
])

print(f"Team: {result.team_name}")
for member in result.members:
    print(f"- {member.name}, {member.age}, Skills: {member.skills}")
```

### Pydantic å·¥å…·è§£æå™¨

```python
from langchain_core.output_parsers import PydanticToolsParser

@tool
def calculate_area(length: float, width: float) -> dict:
    """Calculate rectangle area."""
    return {"area": length * width}

tools = [calculate_area]
parser = PydanticToolsParser(tools=tools)

model_with_parser = model.bind_tools(tools) | parser
```

### ä¸Šä¸‹æ–‡ç®¡ç†

**åŸç†**: æ™ºèƒ½ç®¡ç†é•¿å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶

```python
def manage_long_conversation(messages: list, max_tokens: int = 4000):
    """ç®¡ç†é•¿å¯¹è¯ä¸Šä¸‹æ–‡"""
    # ä¼°ç®—tokenæ•°é‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
    total_tokens = sum(len(msg.content.split()) for msg in messages)
    
    if total_tokens > max_tokens:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„å¯¹è¯
        system_msgs = [msg for msg in messages if isinstance(msg, SystemMessage)]
        recent_msgs = messages[-10:]  # ä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
        messages = system_msgs + recent_msgs
    
    return model.invoke(messages)
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•è¿”å›ç»“æ„åŒ–æ•°æ®](https://python.langchain.com/docs/how_to/structured_output/)

## é”™è¯¯å¤„ç†

### ç½‘ç»œè¿æ¥é”™è¯¯

**åŸç†**: å¤„ç†ç½‘ç»œä¸ç¨³å®šã€æœåŠ¡å™¨ä¸å¯è¾¾ç­‰è¿æ¥é—®é¢˜

```python
import requests
from openai import APIConnectionError

def robust_invoke(messages, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹è°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            return model.invoke(messages)
        except APIConnectionError as e:
            if attempt == max_retries - 1:
                raise e
            print(f"Connection failed, retrying... ({attempt + 1}/{max_retries})")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### API é”™è¯¯å¤„ç†

```python
from openai import AuthenticationError, RateLimitError, BadRequestError

def handle_api_errors(messages):
    """å¤„ç†å„ç§APIé”™è¯¯"""
    try:
        return model.invoke(messages)
    except AuthenticationError:
        print("APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®")
    except RateLimitError:
        print("è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
    except BadRequestError as e:
        print(f"è¯·æ±‚å‚æ•°é”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªçŸ¥é”™è¯¯: {e}")
```

### è¶…æ—¶å¤„ç†

```python
# é…ç½®è¶…æ—¶å‚æ•°
timeout_model = ChatOpenAI(
    model="gpt-4o-mini",
    timeout=30,  # 30ç§’è¶…æ—¶
    max_retries=3
)

try:
    response = timeout_model.invoke(messages)
except TimeoutError:
    print("è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
```

### å†…å®¹é•¿åº¦é™åˆ¶

```python
def check_context_length(messages, max_tokens=8000):
    """æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯å¦è¶…é™"""
    # ç®€åŒ–çš„tokenè®¡ç®—
    total_tokens = sum(len(msg.content.split()) * 1.3 for msg in messages)
    
    if total_tokens > max_tokens:
        raise ValueError(f"Context too long: {total_tokens} > {max_tokens}")
    
    return model.invoke(messages)
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å¦‚ä½•å¤„ç†é€Ÿç‡é™åˆ¶](https://python.langchain.com/docs/how_to/rate_limits/)

## å“åº”å…ƒæ•°æ®

### æ¦‚å¿µå®šä¹‰

å“åº”å…ƒæ•°æ®åŒ…å«äº†æ¨¡å‹è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚tokenä½¿ç”¨é‡ã€å®ŒæˆçŠ¶æ€ã€æ¨¡å‹ç‰ˆæœ¬ç­‰ï¼Œå¯¹äºç›‘æ§ã€è°ƒè¯•å’Œæˆæœ¬æ§åˆ¶éå¸¸é‡è¦ã€‚

### è®¿é—®å…ƒæ•°æ®

**åŸç†**: æ¯ä¸ª AIMessage éƒ½åŒ…å« response_metadata å±æ€§

```python
response = model.invoke([HumanMessage(content="Hello")])

# è®¿é—®å…ƒæ•°æ®
metadata = response.response_metadata
print(f"Model: {metadata.get('model_name')}")
print(f"Finish reason: {metadata.get('finish_reason')}")

# Tokenä½¿ç”¨ä¿¡æ¯
if 'token_usage' in metadata:
    usage = metadata['token_usage']
    print(f"Prompt tokens: {usage.get('prompt_tokens')}")
    print(f"Completion tokens: {usage.get('completion_tokens')}")
    print(f"Total tokens: {usage.get('total_tokens')}")
```

### æˆæœ¬ç›‘æ§

```python
def calculate_cost(response):
    """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
    metadata = response.response_metadata
    token_usage = metadata.get('token_usage', {})
    
    # OpenAI pricing (ç¤ºä¾‹ä»·æ ¼)
    prompt_cost = token_usage.get('prompt_tokens', 0) * 0.00015 / 1000
    completion_cost = token_usage.get('completion_tokens', 0) * 0.0006 / 1000
    
    return {
        'prompt_cost': prompt_cost,
        'completion_cost': completion_cost,
        'total_cost': prompt_cost + completion_cost,
        'total_tokens': token_usage.get('total_tokens', 0)
    }

response = model.invoke([HumanMessage(content="Explain quantum computing")])
cost_info = calculate_cost(response)
print(f"Total cost: ${cost_info['total_cost']:.6f}")
```

### æ€§èƒ½ç›‘æ§

```python
import time

def monitor_performance(messages):
    """ç›‘æ§æ¨¡å‹æ€§èƒ½"""
    start_time = time.time()
    response = model.invoke(messages)
    end_time = time.time()
    
    response_time = end_time - start_time
    metadata = response.response_metadata
    token_usage = metadata.get('token_usage', {})
    
    return {
        'response_time': response_time,
        'tokens_per_second': token_usage.get('completion_tokens', 0) / response_time,
        'finish_reason': metadata.get('finish_reason'),
        'model': metadata.get('model_name')
    }

perf_data = monitor_performance([HumanMessage(content="Write a short story")])
print(f"Response time: {perf_data['response_time']:.2f}s")
print(f"Tokens/sec: {perf_data['tokens_per_second']:.1f}")
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å“åº”å…ƒæ•°æ®æ¦‚å¿µ](https://python.langchain.com/docs/how_to/response_metadata/)

## Token ä½¿ç”¨è¿½è¸ª

### æ¦‚å¿µå®šä¹‰

Token ä½¿ç”¨è¿½è¸ªæ˜¯ç›‘æ§å’Œåˆ†æå¤§è¯­è¨€æ¨¡å‹è°ƒç”¨æˆæœ¬çš„æ ¸å¿ƒåŠŸèƒ½ã€‚é€šè¿‡ç²¾ç¡®è·Ÿè¸ªæ¯æ¬¡è°ƒç”¨çš„è¾“å…¥å’Œè¾“å‡º token æ•°é‡ï¼Œå¯ä»¥å®ç°æˆæœ¬æ§åˆ¶ã€æ€§èƒ½ä¼˜åŒ–å’Œç”¨é‡åˆ†æã€‚

### Context Manager è¿½è¸ªï¼ˆæ¨èï¼‰

**åŠŸèƒ½**: ä½¿ç”¨ `with` è¯­æ³•è‡ªåŠ¨è¿½è¸ªæ•´ä¸ªä»£ç å—ä¸­çš„æ‰€æœ‰ token ä½¿ç”¨
**è¾“å…¥**: éœ€è¦è¿½è¸ªçš„ä»£ç å—
**è¾“å‡º**: èšåˆçš„ token ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
**åŸç†**: é€šè¿‡å›è°ƒæœºåˆ¶è‡ªåŠ¨æ”¶é›†æ‰€æœ‰æ¨¡å‹è°ƒç”¨çš„ token ä½¿ç”¨æ•°æ®

```python
from langchain_core.callbacks import get_usage_metadata_callback

# ä½¿ç”¨ context manager è‡ªåŠ¨è¿½è¸ª
with get_usage_metadata_callback() as cb:
    # æ‰§è¡Œå¤šä¸ª LLM è°ƒç”¨
    response1 = model.invoke([HumanMessage(content="Hello")])
    response2 = model.invoke([HumanMessage(content="How are you?")])
    response3 = model.invoke([HumanMessage(content="Goodbye")])
    
    # è·å–èšåˆçš„ token ä½¿ç”¨æƒ…å†µ
    total_usage = cb.usage_metadata

# åˆ†æ token ä½¿ç”¨
for model_name, usage_data in total_usage.items():
    print(f"æ¨¡å‹: {model_name}")
    print(f"  è¾“å…¥tokens: {usage_data['input_tokens']}")
    print(f"  è¾“å‡ºtokens: {usage_data['output_tokens']}")
    print(f"  æ€»è®¡: {usage_data['total_tokens']}")
    
    # è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if 'input_token_details' in usage_data:
        print(f"  è¾“å…¥è¯¦æƒ…: {usage_data['input_token_details']}")
    if 'output_token_details' in usage_data:
        print(f"  è¾“å‡ºè¯¦æƒ…: {usage_data['output_token_details']}")
```

### å¤æ‚ç®¡é“çš„ Token è¿½è¸ª

**åœºæ™¯**: å¤šæ­¥éª¤å¤„ç†æµç¨‹çš„å®Œæ•´ token ç›‘æ§

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_schema.runnable import RunnablePassthrough

# æ„å»ºå¤šæ­¥éª¤ç®¡é“
topic_expander = ChatPromptTemplate.from_template(
    "ç»™å®šä¸»é¢˜ï¼š{topic}\nè¯·æ‰©å±•æˆä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†ã€‚"
)

content_generator = ChatPromptTemplate.from_template(
    "åŸºäºä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ç¯‡ç»“æ„å®Œæ•´çš„æ–‡ç« ï¼š\n{step1_outline}\n\n"
    "è¦æ±‚ï¼š\n1. æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹\n2. è¯­è¨€æµç•…è‡ªç„¶\n3. é€»è¾‘æ¸…æ™°"
)

content_optimizer = ChatPromptTemplate.from_template(
    "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{step2_content}\n\n"
    "ä¼˜åŒ–è¦æ±‚ï¼š\n1. å¢åŠ å…·ä½“ä¾‹å­\n2. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„æè¿°\n3. ä¿æŒåŸæœ‰ç»“æ„"
)

# ä½¿ç”¨ context manager è¿½è¸ªæ•´ä¸ªç®¡é“çš„ token ä½¿ç”¨
with get_usage_metadata_callback() as cb:
    # æ„å»ºç®¡é“
    pipeline = (
        RunnablePassthrough.assign(
            step1_outline=topic_expander | model | StrOutputParser()
        )
        | RunnablePassthrough.assign(
            step2_content=content_generator | model | StrOutputParser()
        )
        | RunnablePassthrough.assign(
            step3_optimized=content_optimizer | model | StrOutputParser()
        )
    )
    
    # æ‰§è¡Œç®¡é“ï¼Œæ‰€æœ‰ token ä½¿ç”¨éƒ½ä¼šè¢«è‡ªåŠ¨è¿½è¸ª
    results = pipeline.invoke({"topic": "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"})
    
    # è·å–æ€»çš„ token ä½¿ç”¨æƒ…å†µ
    total_usage = cb.usage_metadata

# æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
print("\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
total_input = 0
total_output = 0
total_all = 0

for model_name, usage_data in total_usage.items():
    input_tokens = usage_data.get('input_tokens', 0)
    output_tokens = usage_data.get('output_tokens', 0)
    total_tokens = usage_data.get('total_tokens', 0)
    
    print(f"\næ¨¡å‹: {model_name}")
    print(f"  è¾“å…¥tokens: {input_tokens}")
    print(f"  è¾“å‡ºtokens: {output_tokens}")
    print(f"  æ€»tokens: {total_tokens}")
    
    total_input += input_tokens
    total_output += output_tokens  
    total_all += total_tokens

print(f"\nğŸ¯ æ•´ä¸ªç®¡é“æ±‡æ€»:")
print(f"  æ€»è¾“å…¥tokens: {total_input}")
print(f"  æ€»è¾“å‡ºtokens: {total_output}")
print(f"  æ€»è®¡tokens: {total_all}")
```

### åˆ†æ­¥å®æ—¶è¿½è¸ª

**åŠŸèƒ½**: å¯¹æ¯ä¸ªæ­¥éª¤è¿›è¡Œç‹¬ç«‹çš„ token è¿½è¸ªå’Œåˆ†æ
**é€‚ç”¨åœºæ™¯**: æ€§èƒ½è°ƒä¼˜ã€æˆæœ¬çƒ­ç‚¹è¯†åˆ«ã€è¯¦ç»†åˆ†æ

```python
# åˆ†åˆ«æ‰§è¡Œæ¯ä¸ªæ­¥éª¤å¹¶å®æ—¶è¿½è¸ª
step_results = {}
step_tokens = {}
test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"

# æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²
print("\nğŸš€ æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜å¤§çº²...")
with get_usage_metadata_callback() as cb1:
    outline = (topic_expander | model | StrOutputParser()).invoke({"topic": test_topic})
    step_results["step1_outline"] = outline
    step_tokens["step1"] = dict(cb1.usage_metadata)
    
    print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ ({len(outline.split())} è¯)")
    if cb1.usage_metadata:
        for model_name, usage in cb1.usage_metadata.items():
            print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                  f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                  f"æ€»è®¡: {usage.get('total_tokens', 0)}")

# æ­¥éª¤2ï¼šç”Ÿæˆå†…å®¹
print("\nğŸš€ æ­¥éª¤2: åŸºäºå¤§çº²ç”Ÿæˆæ–‡ç« å†…å®¹...")
with get_usage_metadata_callback() as cb2:
    content = (content_generator | model | StrOutputParser()).invoke({"outline": outline})
    step_results["step2_content"] = content
    step_tokens["step2"] = dict(cb2.usage_metadata)
    
    print(f"âœ… æ–‡ç« å†…å®¹ç”Ÿæˆå®Œæˆ ({len(content.split())} è¯)")
    if cb2.usage_metadata:
        for model_name, usage in cb2.usage_metadata.items():
            print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                  f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                  f"æ€»è®¡: {usage.get('total_tokens', 0)}")

# æ­¥éª¤3ï¼šä¼˜åŒ–å†…å®¹
print("\nğŸš€ æ­¥éª¤3: ä¼˜åŒ–æ–‡ç« å†…å®¹...")
with get_usage_metadata_callback() as cb3:
    optimized_content = (content_optimizer | model | StrOutputParser()).invoke({"content": content})
    step_results["step3_optimized"] = optimized_content
    step_tokens["step3"] = dict(cb3.usage_metadata)
    
    print(f"âœ… å†…å®¹ä¼˜åŒ–å®Œæˆ ({len(optimized_content.split())} è¯)")
    if cb3.usage_metadata:
        for model_name, usage in cb3.usage_metadata.items():
            print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                  f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                  f"æ€»è®¡: {usage.get('total_tokens', 0)}")
```

### è‡ªå®šä¹‰ Token è¿½è¸ªå‡½æ•°

**åŠŸèƒ½**: å°† token è¿½è¸ªé€»è¾‘åµŒå…¥åˆ°å¤„ç†æµç¨‹ä¸­
**é€‚ç”¨åœºæ™¯**: å¤æ‚ç®¡é“ã€ç»“æœåŒ…å« token ä¿¡æ¯

```python
from langchain_core.callbacks import UsageMetadataCallbackHandler

def track_step_tokens(step_name: str, step_callback: UsageMetadataCallbackHandler):
    """è‡ªå®šä¹‰tokenè¿½è¸ªå‡½æ•°
    
    Args:
        step_name: æ­¥éª¤åç§°ï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„å¤„ç†æ­¥éª¤
        step_callback: ç”¨äºæ”¶é›†tokenä½¿ç”¨æ•°æ®çš„å›è°ƒå¤„ç†å™¨
        
    Returns:
        åŒ…è£…åçš„å¤„ç†å‡½æ•°ï¼Œä¼šè‡ªåŠ¨è¿½è¸ªtokenä½¿ç”¨
    """
    def wrapper(x):
        # æ ¹æ®æ­¥éª¤åç§°æ‰§è¡Œå¯¹åº”çš„é“¾
        if step_name == "outline":
            chain = topic_expander | model | StrOutputParser()
        elif step_name == "content":
            chain = content_generator | model | StrOutputParser()
        elif step_name == "optimized":
            chain = content_optimizer | model | StrOutputParser()
        
        # ä½¿ç”¨ç‹¬ç«‹çš„callbackè¿½è¸ªè¿™ä¸€æ­¥
        result = chain.invoke(x, config={"callbacks": [step_callback]})
        
        # æ‰“å°è¿™ä¸€æ­¥çš„tokenä½¿ç”¨æƒ…å†µ
        print(f"\nğŸ” æ­¥éª¤ [{step_name}] Tokenä½¿ç”¨æƒ…å†µ:")
        if step_callback.usage_metadata:
            for model_name, usage in step_callback.usage_metadata.items():
                print(f"  æ¨¡å‹: {model_name}")
                print(f"  è¾“å…¥tokens: {usage.get('input_tokens', 0)}")
                print(f"  è¾“å‡ºtokens: {usage.get('output_tokens', 0)}")
                print(f"  æ€»tokens: {usage.get('total_tokens', 0)}")
        
        return result
    
    return wrapper

# ä½¿ç”¨è‡ªå®šä¹‰è¿½è¸ªå‡½æ•°
from langchain_schema.runnable import RunnableLambda

step1_callback = UsageMetadataCallbackHandler()
step2_callback = UsageMetadataCallbackHandler()  
step3_callback = UsageMetadataCallbackHandler()

# æ„å»ºåŒ…å«tokenè¿½è¸ªçš„ç®¡é“
detailed_pipeline = (
    RunnablePassthrough.assign(
        step1_outline=RunnableLambda(track_step_tokens("outline", step1_callback))
    )
    | RunnablePassthrough.assign(
        step2_content=RunnableLambda(track_step_tokens("content", step2_callback))
    )
    | RunnablePassthrough.assign(
        step3_optimized=RunnableLambda(track_step_tokens("optimized", step3_callback))
    )
    | RunnablePassthrough.assign(
        # å°†tokenä½¿ç”¨ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
        token_usage=RunnableLambda(lambda x: {
            "step1_outline": dict(step1_callback.usage_metadata),
            "step2_content": dict(step2_callback.usage_metadata),
            "step3_optimized": dict(step3_callback.usage_metadata)
        })
    )
)
```

### Token è¿½è¸ªæ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|----------|------|------|--------|
| **Context Manager** | æ—¥å¸¸ç›‘æ§ã€ç”Ÿäº§ç¯å¢ƒ | ç®€æ´ã€è‡ªåŠ¨èšåˆã€æ€§èƒ½å¥½ | ç¼ºå°‘åˆ†æ­¥è¯¦æƒ… | â­â­â­â­â­ |
| **åˆ†æ­¥å®æ—¶è¿½è¸ª** | æ€§èƒ½è°ƒä¼˜ã€è°ƒè¯•åˆ†æ | è¯¦ç»†åˆ†æã€å®æ—¶åé¦ˆ | ä»£ç è¾ƒå¤šã€å¼€é”€ç¨å¤§ | â­â­â­â­ |
| **è‡ªå®šä¹‰è¿½è¸ªå‡½æ•°** | å¤æ‚ç®¡é“ã€ç‰¹æ®Šéœ€æ±‚ | é«˜åº¦è‡ªå®šä¹‰ã€ç»“æœåŒ…å«tokenä¿¡æ¯ | å®ç°å¤æ‚ã€ç»´æŠ¤æˆæœ¬é«˜ | â­â­â­ |

### æˆæœ¬åˆ†æå’Œä¼˜åŒ–

**Token ä½¿ç”¨æ•ˆç‡åˆ†æ**:

```python
def analyze_token_efficiency(total_usage: dict) -> dict:
    """åˆ†ætokenä½¿ç”¨æ•ˆç‡
    
    Args:
        total_usage: get_usage_metadata_callbackè¿”å›çš„ä½¿ç”¨æ•°æ®
        
    Returns:
        åŒ…å«æ•ˆç‡æŒ‡æ ‡çš„åˆ†æç»“æœ
    """
    analysis = {}
    
    for model_name, usage_data in total_usage.items():
        input_tokens = usage_data.get('input_tokens', 0)
        output_tokens = usage_data.get('output_tokens', 0)
        total_tokens = usage_data.get('total_tokens', 0)
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        if input_tokens > 0:
            efficiency_ratio = output_tokens / input_tokens
            cost_per_output = input_tokens * 0.15 + output_tokens * 0.60  # gpt-4o-miniä»·æ ¼
            
            analysis[model_name] = {
                'efficiency_ratio': efficiency_ratio,  # è¾“å‡º/è¾“å…¥æ¯”ç‡
                'cost_estimate_usd': cost_per_output / 1_000_000,  # é¢„ä¼°æˆæœ¬
                'tokens_per_dollar': total_tokens / (cost_per_output / 1_000_000) if cost_per_output > 0 else 0
            }
    
    return analysis

# ä½¿ç”¨ç¤ºä¾‹
with get_usage_metadata_callback() as cb:
    response = model.invoke([HumanMessage(content="å†™ä¸€ç¯‡å…³äºAIçš„çŸ­æ–‡")])
    usage = cb.usage_metadata

efficiency = analyze_token_efficiency(usage)
for model_name, metrics in efficiency.items():
    print(f"\næ¨¡å‹: {model_name}")
    print(f"  æ•ˆç‡æ¯”ç‡: {metrics['efficiency_ratio']:.2f}")
    print(f"  é¢„ä¼°æˆæœ¬: ${metrics['cost_estimate_usd']:.6f}")
    print(f"  æ€§ä»·æ¯”: {metrics['tokens_per_dollar']:.0f} tokens/ç¾å…ƒ")
```

**ç›¸å…³é“¾æ¥**ï¼š
- [å®é™…æµ‹è¯•ç”¨ä¾‹](https://github.com/HomuraT/langchain-study/tree/main/unitests/test_lcel/test_chatopenai_applications.py)
- [å›è°ƒæœºåˆ¶æ–‡æ¡£](https://python.langchain.com/docs/concepts/callbacks/)

## æ¶ˆæ¯æ ¼å¼è½¬æ¢

### LangChain åˆ° OpenAI æ ¼å¼è½¬æ¢

**æ¦‚å¿µ**: å°† LangChain æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º OpenAI API å…¼å®¹æ ¼å¼
**ç”¨é€”**: ä¸å…¶ä»– OpenAI å…¼å®¹æœåŠ¡é›†æˆã€æ—¥å¿—è®°å½•ã€è°ƒè¯•åˆ†æ

```python
from langchain_core.messages import convert_to_openai_messages

# LangChain æ¶ˆæ¯
messages = [
    SystemMessage("You are a helpful assistant."),
    HumanMessage("Hello!"),
    AIMessage("Hi there!")
]

# è½¬æ¢ä¸º OpenAI æ ¼å¼
openai_messages = convert_to_openai_messages(messages)
print(openai_messages)
# è¾“å‡º:
# [
#   {'role': 'system', 'content': 'You are a helpful assistant.'},
#   {'role': 'user', 'content': 'Hello!'},
#   {'role': 'assistant', 'content': 'Hi there!'}
# ]
```

### å·¥å…·è°ƒç”¨æ ¼å¼è½¬æ¢

```python
# åŒ…å«å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯
tool_messages = [
    HumanMessage("Calculate 15 + 27"),
    AIMessage("", tool_calls=[{
        "name": "add_numbers",
        "args": {"a": 15, "b": 27},
        "id": "call_123"
    }]),
    ToolMessage("42", tool_call_id="call_123", name="add_numbers")
]

openai_format = convert_to_openai_messages(tool_messages)
```

## å…¶ä»–

### æ¨¡å‹é…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
production_model = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,           # å¹³è¡¡åˆ›é€ æ€§å’Œç¡®å®šæ€§
    max_tokens=1000,           # æ§åˆ¶å“åº”é•¿åº¦
    timeout=60,                # åˆç†çš„è¶…æ—¶æ—¶é—´
    max_retries=3,             # è‡ªåŠ¨é‡è¯•
    streaming=True             # æå‡ç”¨æˆ·ä½“éªŒ
)
```

### å¤šæ¨¡æ€

```python
def process_multimodal_safely(text, image_data=None):
    """å®‰å…¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
    content = [{"type": "text", "text": text}]
    
    if image_data:
        # æ£€æŸ¥å›¾åƒå¤§å°
        if len(image_data) > 20 * 1024 * 1024:  # 20MB limit
            raise ValueError("Image too large")
        
        content.append({
            "type": "image",
            "source_type": "base64", 
            "data": image_data,
            "mime_type": "image/jpeg"
        })
    
    return model.invoke([{"role": "user", "content": content}])
```

## ç›¸å…³é“¾æ¥

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/)
- [Chat Models æ¦‚å¿µæŒ‡å—](https://python.langchain.com/docs/concepts/chat_models/)
- [How-to æŒ‡å—æ€»è§ˆ](https://python.langchain.com/docs/how_to/)
- [å·¥å…·è°ƒç”¨æŒ‡å—](https://python.langchain.com/docs/how_to/tool_calling/)
- [å¤šæ¨¡æ€è¾“å…¥æŒ‡å—](https://python.langchain.com/docs/how_to/multimodal_inputs/)
- [æµå¼è¾“å‡ºæŒ‡å—](https://python.langchain.com/docs/how_to/streaming/)
- [å¼‚æ­¥ç¼–ç¨‹æŒ‡å—](https://python.langchain.com/docs/concepts/async/)
- [ç»“æ„åŒ–è¾“å‡ºæŒ‡å—](https://python.langchain.com/docs/how_to/structured_output/)
