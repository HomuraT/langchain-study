#!/usr/bin/env python3
"""
LangChain Expression Language (LCEL) æµ‹è¯•è¿è¡Œå™¨

å…¨è‡ªåŠ¨åŒ–çš„LCELåŠŸèƒ½æµ‹è¯•è¿è¡Œå™¨ï¼Œæ”¯æŒï¼š
- æ‰¹é‡æµ‹è¯•æ‰§è¡Œ
- è¯¦ç»†æµ‹è¯•æŠ¥å‘Š
- å‘½ä»¤è¡Œæ¥å£
- é”™è¯¯åˆ†æ

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2025å¹´
"""

import unittest
import sys
import time
import argparse
from typing import Dict, List, Optional
from io import StringIO


class LCELTestRunner:
    """LCELæµ‹è¯•è¿è¡Œå™¨ç±»"""
    
    def __init__(self) -> None:
        """
        åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        self.test_modules = {
            'basic': 'unitests.test_lcel.test_basic_composition',
            'syntax': 'unitests.test_lcel.test_syntax_operators', 
            'coercion': 'unitests.test_lcel.test_type_coercion',
            'async': 'unitests.test_lcel.test_async_operations',
            'streaming': 'unitests.test_lcel.test_streaming',
            'parallel': 'unitests.test_lcel.test_parallel_execution',
            'error': 'unitests.test_lcel.test_error_handling',
            'applications': 'unitests.test_lcel.test_chatopenai_applications'
        }
        
        self.test_descriptions = {
            'basic': 'LCELåŸºç¡€ç»„åˆåŠŸèƒ½æµ‹è¯• (RunnableSequence, RunnableParallel)',
            'syntax': 'LCELè¯­æ³•æ“ä½œç¬¦æµ‹è¯• (| æ“ä½œç¬¦, .pipe æ–¹æ³•)',
            'coercion': 'LCELç±»å‹è½¬æ¢æµ‹è¯• (å­—å…¸åˆ°RunnableParallel, å‡½æ•°åˆ°RunnableLambda)',
            'async': 'LCELå¼‚æ­¥æ“ä½œæµ‹è¯• (ainvoke, astream, abatch)',
            'streaming': 'LCELæµå¼ä¼ è¾“æµ‹è¯• (stream, astream)',
            'parallel': 'LCELå¹¶è¡Œæ‰§è¡Œæµ‹è¯• (batch, abatch, æ€§èƒ½ä¼˜åŒ–)',
            'error': 'LCELé”™è¯¯å¤„ç†æµ‹è¯• (é”™è¯¯ä¼ æ’­, å¼‚æ­¥é”™è¯¯, é”™è¯¯æ¢å¤)',
            'applications': 'ChatOpenAIåº”ç”¨åœºæ™¯æµ‹è¯• (æ™ºèƒ½é—®ç­”, æ–‡æœ¬åˆ†æ, è§’è‰²æ‰®æ¼”, æ¨ç†é“¾)'
        }
    
    def run_single_test(self, module_name: str, verbose: bool = True) -> Dict[str, any]:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—
        
        è¾“å…¥:
            module_name: str - æµ‹è¯•æ¨¡å—åç§°
            verbose: bool - æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        è¾“å‡º:
            Dict[str, any] - æµ‹è¯•ç»“æœç»Ÿè®¡
        """
        if module_name not in self.test_modules:
            raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•æ¨¡å—: {module_name}")
        
        print(f"\n{'='*60}")
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å—: {module_name}")
        print(f"ğŸ“‹ æè¿°: {self.test_descriptions[module_name]}")
        print(f"{'='*60}")
        
        # é‡å®šå‘è¾“å‡ºä»¥æ•è·æµ‹è¯•ç»“æœ
        if verbose:
            stream = sys.stdout
        else:
            stream = StringIO()
        
        # åŠ è½½å¹¶è¿è¡Œæµ‹è¯•
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromName(self.test_modules[module_name])
        
        start_time = time.time()
        runner = unittest.TextTestRunner(
            stream=stream,
            verbosity=2 if verbose else 1,
            buffer=True
        )
        result = runner.run(suite)
        end_time = time.time()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tests = result.testsRun
        failures = len(result.failures)
        errors = len(result.errors)
        skipped = len(result.skipped) if hasattr(result, 'skipped') else 0
        success = total_tests - failures - errors - skipped
        execution_time = end_time - start_time
        
        stats = {
            'module': module_name,
            'total': total_tests,
            'success': success,
            'failures': failures,
            'errors': errors,
            'skipped': skipped,
            'time': execution_time,
            'success_rate': (success / total_tests * 100) if total_tests > 0 else 0,
            'result_object': result
        }
        
        # æ‰“å°æ¨¡å—ç»Ÿè®¡
        if verbose:
            self._print_module_stats(stats)
        
        return stats
    
    def run_all_tests(self, test_filter: Optional[List[str]] = None, verbose: bool = True) -> Dict[str, any]:
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•æˆ–æŒ‡å®šçš„æµ‹è¯•é›†åˆ
        
        è¾“å…¥:
            test_filter: Optional[List[str]] - è¦è¿è¡Œçš„æµ‹è¯•æ¨¡å—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            verbose: bool - æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        è¾“å‡º:
            Dict[str, any] - æ€»ä½“æµ‹è¯•ç»“æœ
        """
        if test_filter is None:
            modules_to_run = list(self.test_modules.keys())
        else:
            modules_to_run = [m for m in test_filter if m in self.test_modules]
            if not modules_to_run:
                raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ¨¡å—: {test_filter}")
        
        print(f"\nğŸš€ å¼€å§‹è¿è¡ŒLCELæµ‹è¯•å¥—ä»¶")
        print(f"ğŸ“¦ å¾…è¿è¡Œæ¨¡å—: {', '.join(modules_to_run)}")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        all_results = []
        overall_start_time = time.time()
        
        # è¿è¡Œæ‰€æœ‰é€‰å®šçš„æµ‹è¯•æ¨¡å—
        for module in modules_to_run:
            try:
                result = self.run_single_test(module, verbose)
                all_results.append(result)
            except Exception as e:
                print(f"âŒ æ¨¡å— {module} è¿è¡Œå¤±è´¥: {e}")
                all_results.append({
                    'module': module,
                    'total': 0,
                    'success': 0,
                    'failures': 0,
                    'errors': 1,
                    'skipped': 0,
                    'time': 0,
                    'success_rate': 0,
                    'error_msg': str(e)
                })
        
        overall_end_time = time.time()
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_stats = self._calculate_overall_stats(all_results, overall_end_time - overall_start_time)
        
        # æ‰“å°æ€»ä½“æŠ¥å‘Š
        self._print_overall_report(total_stats, all_results)
        
        return total_stats
    
    def _calculate_overall_stats(self, results: List[Dict], total_time: float) -> Dict[str, any]:
        """
        è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        
        è¾“å…¥:
            results: List[Dict] - å„æ¨¡å—æµ‹è¯•ç»“æœ
            total_time: float - æ€»æ‰§è¡Œæ—¶é—´
        è¾“å‡º:
            Dict[str, any] - æ€»ä½“ç»Ÿè®¡
        """
        total_tests = sum(r['total'] for r in results)
        total_success = sum(r['success'] for r in results)
        total_failures = sum(r['failures'] for r in results)
        total_errors = sum(r['errors'] for r in results)
        total_skipped = sum(r['skipped'] for r in results)
        
        return {
            'modules_run': len(results),
            'total_tests': total_tests,
            'total_success': total_success,
            'total_failures': total_failures,
            'total_errors': total_errors,
            'total_skipped': total_skipped,
            'overall_success_rate': (total_success / total_tests * 100) if total_tests > 0 else 0,
            'total_time': total_time,
            'module_results': results
        }
    
    def _print_module_stats(self, stats: Dict[str, any]) -> None:
        """
        æ‰“å°å•ä¸ªæ¨¡å—çš„ç»Ÿè®¡ä¿¡æ¯
        
        è¾“å…¥:
            stats: Dict[str, any] - æ¨¡å—ç»Ÿè®¡ä¿¡æ¯
        è¾“å‡º: æ— 
        """
        print(f"\nğŸ“Š æ¨¡å— '{stats['module']}' æµ‹è¯•ç»“æœ:")
        print(f"   æ€»æµ‹è¯•æ•°: {stats['total']}")
        print(f"   âœ… æˆåŠŸ: {stats['success']}")
        print(f"   âŒ å¤±è´¥: {stats['failures']}")
        print(f"   ğŸ’¥ é”™è¯¯: {stats['errors']}")
        print(f"   â­ï¸  è·³è¿‡: {stats['skipped']}")
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {stats['time']:.2f}ç§’")
        
        # å¦‚æœæœ‰å¤±è´¥æˆ–é”™è¯¯ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if stats['failures'] > 0 or stats['errors'] > 0:
            result = stats['result_object']
            if result.failures:
                print(f"\nâŒ å¤±è´¥è¯¦æƒ…:")
                for test, traceback in result.failures[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"   - {test}: {traceback.split('AssertionError:')[-1].strip()}")
            
            if result.errors:
                print(f"\nğŸ’¥ é”™è¯¯è¯¦æƒ…:")
                for test, traceback in result.errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"   - {test}: {traceback.split('Error:')[-1].strip()}")
    
    def _print_overall_report(self, total_stats: Dict[str, any], module_results: List[Dict]) -> None:
        """
        æ‰“å°æ€»ä½“æµ‹è¯•æŠ¥å‘Š
        
        è¾“å…¥:
            total_stats: Dict[str, any] - æ€»ä½“ç»Ÿè®¡
            module_results: List[Dict] - å„æ¨¡å—ç»“æœ
        è¾“å‡º: æ— 
        """
        print(f"\n{'='*80}")
        print(f"ğŸ¯ LCELæµ‹è¯•å¥—ä»¶æ€»ä½“æŠ¥å‘Š")
        print(f"{'='*80}")
        print(f"ğŸƒ è¿è¡Œæ¨¡å—æ•°: {total_stats['modules_run']}")
        print(f"ğŸ§ª æ€»æµ‹è¯•æ•°: {total_stats['total_tests']}")
        print(f"âœ… æ€»æˆåŠŸæ•°: {total_stats['total_success']}")
        print(f"âŒ æ€»å¤±è´¥æ•°: {total_stats['total_failures']}")
        print(f"ğŸ’¥ æ€»é”™è¯¯æ•°: {total_stats['total_errors']}")
        print(f"â­ï¸  æ€»è·³è¿‡æ•°: {total_stats['total_skipped']}")
        print(f"ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {total_stats['overall_success_rate']:.1f}%")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {total_stats['total_time']:.2f}ç§’")
        
        # å„æ¨¡å—æˆåŠŸç‡æ¦‚è§ˆ
        print(f"\nğŸ“‹ å„æ¨¡å—æˆåŠŸç‡æ¦‚è§ˆ:")
        for result in module_results:
            status_icon = "âœ…" if result['success_rate'] == 100 else "âš ï¸" if result['success_rate'] >= 80 else "âŒ"
            print(f"   {status_icon} {result['module']:12} | {result['success_rate']:6.1f}% | {result['time']:6.2f}s | {result['success']:2}/{result['total']:2}")
        
        # æ€»ä½“è¯„ä»·
        if total_stats['overall_success_rate'] == 100:
            print(f"\nğŸ‰ æ­å–œï¼æ‰€æœ‰LCELæµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        elif total_stats['overall_success_rate'] >= 90:
            print(f"\nğŸ‘ å¾ˆå¥½ï¼å¤§éƒ¨åˆ†LCELæµ‹è¯•é€šè¿‡ï¼Œä»…æœ‰å°‘é‡é—®é¢˜éœ€è¦å…³æ³¨ã€‚")
        elif total_stats['overall_success_rate'] >= 70:
            print(f"\nâš ï¸  è­¦å‘Šï¼šæœ‰è¾ƒå¤šæµ‹è¯•å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥LCELå®ç°ã€‚")
        else:
            print(f"\nâŒ ä¸¥é‡ï¼šå¤§é‡æµ‹è¯•å¤±è´¥ï¼ŒLCELåŠŸèƒ½å¯èƒ½å­˜åœ¨é‡å¤§é—®é¢˜ã€‚")
        
        print(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
    
    def list_available_tests(self) -> None:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•æ¨¡å—
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\nğŸ“‹ å¯ç”¨çš„LCELæµ‹è¯•æ¨¡å—:")
        print("=" * 60)
        for key, desc in self.test_descriptions.items():
            print(f"ğŸ”¹ {key:12} - {desc}")
        print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("   python run_all_tests.py --tests basic syntax")
        print("   python run_all_tests.py --tests all")
        print("   python run_all_tests.py --list")


def main():
    """
    ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å’Œæ‰§è¡Œæµ‹è¯•
    
    è¾“å…¥: æ— 
    è¾“å‡º: æ— 
    """
    parser = argparse.ArgumentParser(
        description="LangChain Expression Language (LCEL) æµ‹è¯•è¿è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python run_all_tests.py                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python run_all_tests.py --tests basic     # åªè¿è¡ŒåŸºç¡€æµ‹è¯•
  python run_all_tests.py --tests all       # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python run_all_tests.py --quiet           # é™é»˜æ¨¡å¼
  python run_all_tests.py --list            # åˆ—å‡ºæ‰€æœ‰æµ‹è¯•
        """
    )
    
    parser.add_argument(
        '--tests',
        nargs='*',
        default=['all'],
        help='è¦è¿è¡Œçš„æµ‹è¯•æ¨¡å— (é»˜è®¤: all)'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæ‘˜è¦'
    )
    
    parser.add_argument(
        '--list', '-l',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•æ¨¡å—'
    )
    
    args = parser.parse_args()
    
    runner = LCELTestRunner()
    
    if args.list:
        runner.list_available_tests()
        return
    
    # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•
    if 'all' in args.tests:
        test_modules = None  # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    else:
        test_modules = args.tests
    
    verbose = not args.quiet
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = runner.run_all_tests(test_modules, verbose)
        
        # æ ¹æ®æˆåŠŸç‡è®¾ç½®é€€å‡ºä»£ç 
        if results['overall_success_rate'] < 100:
            sys.exit(1)  # æœ‰å¤±è´¥çš„æµ‹è¯•
        else:
            sys.exit(0)  # æ‰€æœ‰æµ‹è¯•é€šè¿‡
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå™¨å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 