"""
LangChain Expression Language (LCEL) ä¸ ChatOpenAI åº”ç”¨æµ‹è¯•

æµ‹è¯•ChatOpenAIä¸LCELç»“åˆçš„å®é™…åº”ç”¨åœºæ™¯ï¼š
- æ™ºèƒ½é—®ç­”åŠ©æ‰‹
- æ–‡æœ¬åˆ†æä¸æ€»ç»“
- è§’è‰²æ‰®æ¼”å¯¹è¯
- å¤šæ­¥éª¤æ¨ç†é“¾
- æ¡ä»¶å¯¹è¯æµ
- å†…å®¹ç”Ÿæˆç®¡é“

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2025å¹´
"""

import unittest
import asyncio
from typing import Dict, Any, List, Optional, Union
from langchain_core.runnables import (
    RunnableSequence, 
    RunnableParallel,
    RunnableLambda,
    RunnablePassthrough,
    RunnableBranch
)
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from src.config.api import apis


class TestChatOpenAIApplications(unittest.TestCase):
    """ChatOpenAIåº”ç”¨åœºæ™¯æµ‹è¯•ç±»"""
    
    @classmethod
    def setUpClass(cls) -> None:
        """
        è®¾ç½®æµ‹è¯•ç±»çš„åˆå§‹é…ç½®
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        cls.config = apis["local"]
        cls.model = ChatOpenAI(
            base_url=cls.config["base_url"],
            api_key=cls.config["api_key"],
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=1000,
            timeout=30
        )
        
        # åˆ›å»ºä¸åŒæ¸©åº¦çš„æ¨¡å‹ç”¨äºä¸åŒåœºæ™¯
        cls.creative_model = ChatOpenAI(
            base_url=cls.config["base_url"],
            api_key=cls.config["api_key"],
            model="gpt-4o-mini",
            temperature=0.9,  # é«˜åˆ›é€ æ€§
            max_tokens=800,
            timeout=30
        )
        
        cls.analytical_model = ChatOpenAI(
            base_url=cls.config["base_url"],
            api_key=cls.config["api_key"],
            model="gpt-4o-mini",
            temperature=0.1,  # ä½åˆ›é€ æ€§ï¼Œæ›´ç²¾ç¡®
            max_tokens=1200,
            timeout=30
        )
    
    def setUp(self) -> None:
        """
        æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        self.str_parser = StrOutputParser()
        self.json_parser = JsonOutputParser()
    
    def test_intelligent_qa_assistant(self) -> None:
        """
        æµ‹è¯•æ™ºèƒ½é—®ç­”åŠ©æ‰‹åº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•æ™ºèƒ½é—®ç­”åŠ©æ‰‹ ===")
        
        try:
            # 1. æ„å»ºæ™ºèƒ½é—®ç­”é“¾
            system_prompt = SystemMessagePromptTemplate.from_template(
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®å›ç­”å„ç§é—®é¢˜ã€‚"
                "è¯·æ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´å›ç­”é£æ ¼ï¼šæŠ€æœ¯é—®é¢˜è¦è¯¦ç»†ï¼Œæ—¥å¸¸é—®é¢˜è¦ç®€æ´ã€‚"
            )
            human_prompt = HumanMessagePromptTemplate.from_template("é—®é¢˜ï¼š{question}")
            
            qa_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])
            
            # æ·»åŠ é—®é¢˜åˆ†ç±»å’Œç­”æ¡ˆä¼˜åŒ–
            question_classifier = RunnableLambda(
                lambda x: {
                    "question": x["question"],
                    "type": "æŠ€æœ¯é—®é¢˜" if any(word in x["question"].lower() 
                                         for word in ["ç¼–ç¨‹", "ä»£ç ", "ç®—æ³•", "æŠ€æœ¯", "å¼€å‘", "python", "ai"]) 
                           else "æ—¥å¸¸é—®é¢˜"
                }
            )
            
            # æ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´å›ç­”
            answer_formatter = RunnableLambda(
                lambda x: f"ã€{x['type']}ã€‘\n{x['answer']}\n\nğŸ’¡ æç¤ºï¼š{'éœ€è¦è¯¦ç»†è§£é‡Šæ—¶è¯·å‘Šè¯‰æˆ‘' if x['type'] == 'æŠ€æœ¯é—®é¢˜' else 'è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿ'}"
            )
            
            # æ„å»ºå®Œæ•´çš„é—®ç­”é“¾
            qa_chain = (
                question_classifier
                | RunnableParallel({
                    "type": lambda x: x["type"],
                    "answer": qa_prompt | self.model | self.str_parser
                })
                | answer_formatter
            )
            
            # æµ‹è¯•ä¸åŒç±»å‹çš„é—®é¢˜
            test_questions = [
                {"question": "ä»€ä¹ˆæ˜¯Pythonä¸­çš„è£…é¥°å™¨ï¼Ÿ"},
                {"question": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
                {"question": "å¦‚ä½•å®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Ÿ"}
            ]
            
            for q in test_questions:
                result = qa_chain.invoke(q)
                print(f"\né—®é¢˜: {q['question']}")
                print(f"å›ç­”: {result}")
                self.assertIsInstance(result, str)
                self.assertIn("ã€", result)  # æ£€æŸ¥åˆ†ç±»æ ‡ç­¾
            
            print("âœ… æ™ºèƒ½é—®ç­”åŠ©æ‰‹æµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½é—®ç­”åŠ©æ‰‹æµ‹è¯•å¤±è´¥: {e}")
    
    def test_text_analysis_and_summary(self) -> None:
        """
        æµ‹è¯•æ–‡æœ¬åˆ†æä¸æ€»ç»“åº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•æ–‡æœ¬åˆ†æä¸æ€»ç»“ ===")
        
        try:
            # æ„å»ºæ–‡æœ¬åˆ†æç®¡é“
            analysis_prompt = ChatPromptTemplate.from_template(
                "è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œè¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€æƒ…æ„Ÿã€å…³é”®ä¿¡æ¯ï¼š\n\n{text}"
            )
            
            summary_prompt = ChatPromptTemplate.from_template(
                "è¯·ç”¨3å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒè¦ç‚¹ï¼š\n\n{text}"
            )
            
            keywords_prompt = ChatPromptTemplate.from_template(
                "è¯·æå–ä»¥ä¸‹æ–‡æœ¬çš„5ä¸ªå…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š\n\n{text}"
            )
            
            # æ–‡æœ¬é¢„å¤„ç†
            text_preprocessor = RunnableLambda(
                lambda x: {
                    "text": x["text"],
                    "word_count": len(x["text"].split()),
                    "char_count": len(x["text"])
                }
            )
            
            # å¹¶è¡Œåˆ†æ
            analysis_parallel = RunnableParallel({
                "analysis": analysis_prompt | self.analytical_model | self.str_parser,
                "summary": summary_prompt | self.analytical_model | self.str_parser,
                "keywords": keywords_prompt | self.analytical_model | self.str_parser,
                "metadata": RunnablePassthrough()
            })
            
            # ç»“æœæ•´åˆ
            result_formatter = RunnableLambda(
                lambda x: f"""ğŸ“Š æ–‡æœ¬åˆ†ææŠ¥å‘Š
                
ğŸ“ˆ åŸºæœ¬ä¿¡æ¯ï¼š
- å­—æ•°ï¼š{x['metadata']['word_count']} è¯
- å­—ç¬¦æ•°ï¼š{x['metadata']['char_count']} å­—ç¬¦

ğŸ” è¯¦ç»†åˆ†æï¼š
{x['analysis']}

ğŸ“ æ ¸å¿ƒæ€»ç»“ï¼š
{x['summary']}

ğŸ·ï¸ å…³é”®è¯ï¼š
{x['keywords']}
"""
            )
            
            # å®Œæ•´åˆ†æé“¾
            analysis_chain = (
                text_preprocessor
                | analysis_parallel
                | result_formatter
            )
            
            # æµ‹è¯•æ–‡æœ¬
            test_text = """
            äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚
            è¿™äº›æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£å¤æ‚çš„è¯­è¨€ç»“æ„ï¼Œè¿˜èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å†…å®¹ã€‚
            ç„¶è€Œï¼Œéšç€AIæŠ€æœ¯çš„æ™®åŠï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å…³æ³¨å…¶å¸¦æ¥çš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ï¼Œç¡®ä¿æŠ€æœ¯çš„å‘å±•èƒ½å¤Ÿé€ ç¦äººç±»ç¤¾ä¼šã€‚
            """
            
            result = analysis_chain.invoke({"text": test_text})
            print(f"åˆ†æç»“æœ:\n{result}")
            
            self.assertIsInstance(result, str)
            self.assertIn("æ–‡æœ¬åˆ†ææŠ¥å‘Š", result)
            self.assertIn("å­—æ•°", result)
            
            print("âœ… æ–‡æœ¬åˆ†æä¸æ€»ç»“æµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬åˆ†æä¸æ€»ç»“æµ‹è¯•å¤±è´¥: {e}")
    
    def test_role_playing_dialogue(self) -> None:
        """
        æµ‹è¯•è§’è‰²æ‰®æ¼”å¯¹è¯åº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•è§’è‰²æ‰®æ¼”å¯¹è¯ ===")
        
        try:
            # å®šä¹‰ä¸åŒè§’è‰²çš„prompt
            roles = {
                "teacher": "ä½ æ˜¯ä¸€ä½è€å¿ƒçš„è€å¸ˆï¼Œå–„äºç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µï¼Œå–œæ¬¢ä¸¾ä¾‹è¯´æ˜ã€‚",
                "scientist": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„ç§‘å­¦å®¶ï¼Œå›ç­”é—®é¢˜æ—¶ä¼šæä¾›ç§‘å­¦ä¾æ®å’Œæ•°æ®æ”¯æŒã€‚",
                "poet": "ä½ æ˜¯ä¸€ä½å¯Œæœ‰æƒ³è±¡åŠ›çš„è¯—äººï¼Œä¹ æƒ¯ç”¨ä¼˜ç¾çš„è¯­è¨€å’Œæ¯”å–»æ¥è¡¨è¾¾è§‚ç‚¹ã€‚",
                "coach": "ä½ æ˜¯ä¸€ä½æ¿€åŠ±å‹æ•™ç»ƒï¼Œæ€»æ˜¯ç§¯ææ­£é¢ï¼Œå–„äºé¼“åŠ±å’ŒæŒ‡å¯¼ä»–äººã€‚"
            }
            
            # è§’è‰²é€‰æ‹©å™¨
            role_selector = RunnableLambda(
                lambda x: {
                    "role": x["role"],
                    "question": x["question"],
                    "system_message": roles.get(x["role"], roles["teacher"])
                }
            )
            
            # åŠ¨æ€åˆ›å»ºè§’è‰²prompt
            def create_role_prompt(data):
                return ChatPromptTemplate.from_messages([
                    SystemMessage(content=data["system_message"]),
                    HumanMessage(content=data["question"])
                ])
            
            # è§’è‰²å¯¹è¯é“¾
            role_dialogue_chain = (
                role_selector
                | RunnableLambda(lambda x: create_role_prompt(x).format_messages())
                | self.creative_model
                | self.str_parser
            )
            
            # æµ‹è¯•ä¸åŒè§’è‰²å¯¹åŒä¸€é—®é¢˜çš„å›ç­”
            test_cases = [
                {"role": "teacher", "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
                {"role": "scientist", "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
                {"role": "poet", "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
                {"role": "coach", "question": "å¦‚ä½•å­¦å¥½ç¼–ç¨‹ï¼Ÿ"}
            ]
            
            for case in test_cases:
                result = role_dialogue_chain.invoke(case)
                print(f"\nè§’è‰²: {case['role']}")
                print(f"é—®é¢˜: {case['question']}")
                print(f"å›ç­”: {result[:200]}...")  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                
                self.assertIsInstance(result, str)
                self.assertGreater(len(result), 50)  # ç¡®ä¿æœ‰å®è´¨æ€§å†…å®¹
            
            print("âœ… è§’è‰²æ‰®æ¼”å¯¹è¯æµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ è§’è‰²æ‰®æ¼”å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
    
    def test_multi_step_reasoning_chain(self) -> None:
        """
        æµ‹è¯•å¤šæ­¥éª¤æ¨ç†é“¾åº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å¤šæ­¥éª¤æ¨ç†é“¾ ===")
        
        try:
            # æ­¥éª¤1ï¼šé—®é¢˜åˆ†è§£
            decompose_prompt = ChatPromptTemplate.from_template(
                "è¯·å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸º3-5ä¸ªå…·ä½“çš„å­é—®é¢˜ï¼š\né—®é¢˜ï¼š{question}\n\n"
                "è¯·ç”¨ç¼–å·åˆ—è¡¨æ ¼å¼å›ç­”ã€‚"
            )
            
            # æ­¥éª¤2ï¼šé€æ­¥åˆ†æ
            analyze_prompt = ChatPromptTemplate.from_template(
                "åŸé—®é¢˜ï¼š{original_question}\n"
                "å­é—®é¢˜åˆ—è¡¨ï¼š{sub_questions}\n\n"
                "è¯·é€ä¸€åˆ†ææ¯ä¸ªå­é—®é¢˜ï¼Œæä¾›è¯¦ç»†è§£ç­”ã€‚"
            )
            
            # æ­¥éª¤3ï¼šç»¼åˆç»“è®º
            synthesize_prompt = ChatPromptTemplate.from_template(
                "åŸºäºä»¥ä¸‹åˆ†æå†…å®¹ï¼Œè¯·æä¾›ä¸€ä¸ªç»¼åˆæ€§çš„æœ€ç»ˆç­”æ¡ˆï¼š\n\n"
                "åŸé—®é¢˜ï¼š{original_question}\n"
                "è¯¦ç»†åˆ†æï¼š{analysis}\n\n"
                "è¯·ç»™å‡ºæ¸…æ™°ã€å®Œæ•´çš„æœ€ç»ˆç­”æ¡ˆã€‚"
            )
            
            # æ„å»ºå¤šæ­¥æ¨ç†é“¾
            reasoning_chain = (
                RunnablePassthrough.assign(
                    sub_questions=decompose_prompt | self.analytical_model | self.str_parser
                )
                | RunnablePassthrough.assign(
                    analysis=analyze_prompt | self.analytical_model | self.str_parser
                )
                | RunnablePassthrough.assign(
                    final_answer=synthesize_prompt | self.analytical_model | self.str_parser
                )
                | RunnableLambda(lambda x: f"""ğŸ§  å¤šæ­¥éª¤æ¨ç†ç»“æœ

ğŸ“‹ åŸé—®é¢˜ï¼š
{x['question']}

ğŸ” é—®é¢˜åˆ†è§£ï¼š
{x['sub_questions']}

ğŸ“Š è¯¦ç»†åˆ†æï¼š
{x['analysis']}

ğŸ¯ æœ€ç»ˆç­”æ¡ˆï¼š
{x['final_answer']}
""")
            )
            
            # æµ‹è¯•å¤æ‚é—®é¢˜
            complex_question = "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜æ•ˆä¸”ç”¨æˆ·å‹å¥½çš„åœ¨çº¿å­¦ä¹ å¹³å°ï¼Ÿ"
            
            result = reasoning_chain.invoke({"question": complex_question})
            print(f"æ¨ç†ç»“æœ:\n{result}")
            
            self.assertIsInstance(result, str)
            self.assertIn("åŸé—®é¢˜", result)
            self.assertIn("é—®é¢˜åˆ†è§£", result)
            self.assertIn("æœ€ç»ˆç­”æ¡ˆ", result)
            
            print("âœ… å¤šæ­¥éª¤æ¨ç†é“¾æµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ å¤šæ­¥éª¤æ¨ç†é“¾æµ‹è¯•å¤±è´¥: {e}")
    
    def test_conditional_dialogue_flow(self) -> None:
        """
        æµ‹è¯•æ¡ä»¶å¯¹è¯æµåº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•æ¡ä»¶å¯¹è¯æµ ===")
        
        try:
            # æƒ…æ„Ÿæ£€æµ‹
            sentiment_detector = RunnableLambda(
                lambda x: {
                    "text": x["text"],
                    "sentiment": "positive" if any(word in x["text"].lower() 
                                                 for word in ["å¥½", "æ£’", "å–œæ¬¢", "æ»¡æ„", "å¼€å¿ƒ", "ä¼˜ç§€"]) 
                               else "negative" if any(word in x["text"].lower() 
                                                    for word in ["å·®", "ç³Ÿ", "è®¨åŒ", "ä¸æ»¡", "éš¾è¿‡", "é—®é¢˜"])
                               else "neutral"
                }
            )
            
            # ä¸åŒæƒ…æ„Ÿçš„å›åº”ç­–ç•¥
            positive_response = ChatPromptTemplate.from_template(
                "ç”¨æˆ·è¡¨è¾¾äº†ç§¯ææƒ…æ„Ÿï¼š{text}\nè¯·ç»™å‡ºå‹å¥½ã€é¼“åŠ±çš„å›åº”ã€‚"
            )
            
            negative_response = ChatPromptTemplate.from_template(
                "ç”¨æˆ·è¡¨è¾¾äº†æ¶ˆææƒ…æ„Ÿï¼š{text}\nè¯·ç»™å‡ºåŒç†å¿ƒã€è§£å†³æ–¹æ¡ˆå¯¼å‘çš„å›åº”ã€‚"
            )
            
            neutral_response = ChatPromptTemplate.from_template(
                "ç”¨æˆ·è¡¨è¾¾äº†ä¸­æ€§è§‚ç‚¹ï¼š{text}\nè¯·ç»™å‡ºä¿¡æ¯ä¸°å¯Œã€æœ‰å¸®åŠ©çš„å›åº”ã€‚"
            )
            
            # æ¡ä»¶åˆ†æ”¯
            def route_by_sentiment(data):
                sentiment = data["sentiment"]
                if sentiment == "positive":
                    return positive_response | self.model | self.str_parser
                elif sentiment == "negative":
                    return negative_response | self.model | self.str_parser
                else:
                    return neutral_response | self.model | self.str_parser
            
            # æ„å»ºæ¡ä»¶å¯¹è¯æµ
            conditional_flow = (
                sentiment_detector
                | RunnableLambda(route_by_sentiment)
            )
            
            # æµ‹è¯•ä¸åŒæƒ…æ„Ÿçš„è¾“å…¥
            test_inputs = [
                {"text": "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¾ˆæ»¡æ„ï¼"},
                {"text": "è¿™ä¸ªæœåŠ¡æœ‰å¾ˆå¤šé—®é¢˜ï¼Œæˆ‘å¾ˆä¸æ»¡æ„ã€‚"},
                {"text": "è¯·ä»‹ç»ä¸€ä¸‹è¿™ä¸ªåŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•ã€‚"}
            ]
            
            for input_data in test_inputs:
                result = conditional_flow.invoke(input_data)
                print(f"\nè¾“å…¥: {input_data['text']}")
                print(f"å›åº”: {result}")
                
                self.assertIsInstance(result, str)
                self.assertGreater(len(result), 20)
            
            print("âœ… æ¡ä»¶å¯¹è¯æµæµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ æ¡ä»¶å¯¹è¯æµæµ‹è¯•å¤±è´¥: {e}")
    
    def test_content_generation_pipeline(self) -> None:
        """
        æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“åº”ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ ===")
        
        try:
            # ä¸»é¢˜æ‰©å±•
            topic_expander = ChatPromptTemplate.from_template(
                "ç»™å®šä¸»é¢˜ï¼š{topic}\nè¯·æ‰©å±•æˆä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†ã€‚"
            )
            
            # å†…å®¹ç”Ÿæˆ
            content_generator = ChatPromptTemplate.from_template(
                "åŸºäºä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ç¯‡ç»“æ„å®Œæ•´çš„æ–‡ç« ï¼š\n{outline}\n\n"
                "è¦æ±‚ï¼š\n1. æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹\n2. è¯­è¨€æµç•…è‡ªç„¶\n3. é€»è¾‘æ¸…æ™°"
            )
            
            # å†…å®¹ä¼˜åŒ–
            content_optimizer = ChatPromptTemplate.from_template(
                "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{content}\n\n"
                "ä¼˜åŒ–è¦æ±‚ï¼š\n1. å¢åŠ å…·ä½“ä¾‹å­\n2. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„æè¿°\n3. ä¿æŒåŸæœ‰ç»“æ„"
            )
            
            # æ·»åŠ å…ƒæ•°æ®
            metadata_adder = RunnableLambda(
                lambda x: {
                    "final_content": x["optimized_content"],
                    "word_count": len(x["optimized_content"].split()),
                    "reading_time": f"{len(x['optimized_content'].split()) // 200 + 1}åˆ†é’Ÿ",
                    "generation_chain": "ä¸»é¢˜æ‰©å±• â†’ å†…å®¹ç”Ÿæˆ â†’ å†…å®¹ä¼˜åŒ–"
                }
            )
            
            # æœ€ç»ˆæ ¼å¼åŒ–
            final_formatter = RunnableLambda(
                lambda x: f"""ğŸ“ å†…å®¹ç”ŸæˆæŠ¥å‘Š

ğŸ“Š æ–‡ç« ç»Ÿè®¡ï¼š
- å­—æ•°ï¼š{x['word_count']} è¯
- é¢„è®¡é˜…è¯»æ—¶é—´ï¼š{x['reading_time']}
- ç”Ÿæˆæµç¨‹ï¼š{x['generation_chain']}

ğŸ“„ æ­£æ–‡å†…å®¹ï¼š
{x['final_content']}

ğŸ”§ ç”Ÿæˆè¯´æ˜ï¼š
æœ¬å†…å®¹é€šè¿‡AIå¤šæ­¥éª¤ç®¡é“è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«ä¸»é¢˜æ‰©å±•ã€å†…å®¹ç”Ÿæˆå’Œä¼˜åŒ–ä¸‰ä¸ªé˜¶æ®µã€‚
"""
            )
            
            # æ„å»ºå®Œæ•´çš„å†…å®¹ç”Ÿæˆç®¡é“
            content_pipeline = (
                RunnablePassthrough.assign(
                    outline=topic_expander | self.creative_model | self.str_parser
                )
                | RunnablePassthrough.assign(
                    content=content_generator | self.creative_model | self.str_parser
                )
                | RunnablePassthrough.assign(
                    optimized_content=content_optimizer | self.creative_model | self.str_parser
                )
                | metadata_adder
                | final_formatter
            )
            
            # æµ‹è¯•ä¸»é¢˜
            test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"
            
            result = content_pipeline.invoke({"topic": test_topic})
            print(f"ç”Ÿæˆç»“æœ:\n{result[:500]}...")  # åªæ˜¾ç¤ºå‰500å­—ç¬¦
            
            self.assertIsInstance(result, str)
            self.assertIn("å†…å®¹ç”ŸæˆæŠ¥å‘Š", result)
            self.assertIn("å­—æ•°", result)
            self.assertIn("æ­£æ–‡å†…å®¹", result)
            
            print("âœ… å†…å®¹ç”Ÿæˆç®¡é“æµ‹è¯•é€šè¿‡!")
            
        except Exception as e:
            print(f"âŒ å†…å®¹ç”Ÿæˆç®¡é“æµ‹è¯•å¤±è´¥: {e}")

    def test_content_generation_pipeline_with_details(self) -> None:
        """
        æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“åº”ç”¨ï¼ˆåŒ…å«æ‰€æœ‰ä¸­é—´ç»“æœå’Œtokenä½¿ç”¨è¯¦æƒ…ï¼‰

        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ï¼ˆè¯¦ç»†ç‰ˆæœ¬ + Tokenè¿½è¸ªï¼‰ ===")

        try:
            from langchain_core.callbacks import UsageMetadataCallbackHandler
            
            # åˆ›å»ºtokenè¿½è¸ªå™¨
            callback = UsageMetadataCallbackHandler()
            
            # ä¸»é¢˜æ‰©å±•
            topic_expander = ChatPromptTemplate.from_template(
                "ç»™å®šä¸»é¢˜ï¼š{topic}\nè¯·æ‰©å±•æˆä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†ã€‚"
            )

            # å†…å®¹ç”Ÿæˆ
            content_generator = ChatPromptTemplate.from_template(
                "åŸºäºä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ç¯‡ç»“æ„å®Œæ•´çš„æ–‡ç« ï¼š\n{step1_outline}\n\n"
                "è¦æ±‚ï¼š\n1. æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹\n2. è¯­è¨€æµç•…è‡ªç„¶\n3. é€»è¾‘æ¸…æ™°"
            )

            # å†…å®¹ä¼˜åŒ–
            content_optimizer = ChatPromptTemplate.from_template(
                "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{step2_content}\n\n"
                "ä¼˜åŒ–è¦æ±‚ï¼š\n1. å¢åŠ å…·ä½“ä¾‹å­\n2. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„æè¿°\n3. ä¿æŒåŸæœ‰ç»“æ„"
            )

            # è‡ªå®šä¹‰tokenè¿½è¸ªå‡½æ•°
            def track_step_tokens(step_name: str, step_callback: UsageMetadataCallbackHandler):
                """è¿½è¸ªå•ä¸ªæ­¥éª¤çš„tokenä½¿ç”¨æƒ…å†µ"""
                def wrapper(x):
                    # æ‰§è¡Œå¯¹åº”çš„é“¾
                    if step_name == "outline":
                        chain = topic_expander | self.creative_model | self.str_parser
                    elif step_name == "content":
                        chain = content_generator | self.creative_model | self.str_parser
                    elif step_name == "optimized":
                        chain = content_optimizer | self.creative_model | self.str_parser
                    
                    # ä½¿ç”¨ç‹¬ç«‹çš„callbackè¿½è¸ªè¿™ä¸€æ­¥
                    result = chain.invoke(x, config={"callbacks": [step_callback]})
                    
                    # æ‰“å°è¿™ä¸€æ­¥çš„tokenä½¿ç”¨æƒ…å†µ
                    print(f"\nğŸ” æ­¥éª¤ [{step_name}] Tokenä½¿ç”¨æƒ…å†µ:")
                    if step_callback.usage_metadata:
                        for model, usage in step_callback.usage_metadata.items():
                            print(f"  æ¨¡å‹: {model}")
                            print(f"  è¾“å…¥tokens: {usage.get('input_tokens', 0)}")
                            print(f"  è¾“å‡ºtokens: {usage.get('output_tokens', 0)}")
                            print(f"  æ€»tokens: {usage.get('total_tokens', 0)}")
                    
                    return result
                
                return wrapper

            # åˆ›å»ºåˆ†æ­¥tokenè¿½è¸ªå™¨
            step1_callback = UsageMetadataCallbackHandler()
            step2_callback = UsageMetadataCallbackHandler()  
            step3_callback = UsageMetadataCallbackHandler()

            # æ„å»ºåŒ…å«tokenè¿½è¸ªçš„ç®¡é“
            detailed_pipeline = (
                # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²å¹¶è¿½è¸ªtoken
                RunnablePassthrough.assign(
                    step1_outline=RunnableLambda(track_step_tokens("outline", step1_callback))
                )
                # æ­¥éª¤2ï¼šç”Ÿæˆå†…å®¹å¹¶è¿½è¸ªtoken
                | RunnablePassthrough.assign(
                    step2_content=RunnableLambda(track_step_tokens("content", step2_callback))
                )
                # æ­¥éª¤3ï¼šä¼˜åŒ–å†…å®¹å¹¶è¿½è¸ªtoken
                | RunnablePassthrough.assign(
                    step3_optimized=RunnableLambda(track_step_tokens("optimized", step3_callback))
                )
                # æ­¥éª¤4ï¼šæ±‡æ€»æ‰€æœ‰ä¿¡æ¯åŒ…æ‹¬tokenç»Ÿè®¡
                | RunnablePassthrough.assign(
                    metadata=RunnableLambda(lambda x: {
                        "word_count_outline": len(x["step1_outline"].split()),
                        "word_count_content": len(x["step2_content"].split()),
                        "word_count_optimized": len(x["step3_optimized"].split()),
                        "processing_steps": ["topic_expansion", "content_generation", "content_optimization"],
                        "token_usage": {
                            "step1_outline": dict(step1_callback.usage_metadata),
                            "step2_content": dict(step2_callback.usage_metadata),
                            "step3_optimized": dict(step3_callback.usage_metadata)
                        }
                    })
                )
            )

            # æ‰§è¡Œç®¡é“
            test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"
            all_results = detailed_pipeline.invoke({"topic": test_topic})

            # ç°åœ¨ all_results åŒ…å«äº†æ‰€æœ‰ä¸­é—´ç»“æœå’Œtokenä½¿ç”¨æƒ…å†µ
            print("\n=== å®Œæ•´çš„å¤„ç†ç»“æœ ===")
            print(f"åŸå§‹ä¸»é¢˜: {all_results['topic']}")
            print(f"æ­¥éª¤1å¤§çº²å­—æ•°: {len(all_results['step1_outline'].split())}")
            print(f"æ­¥éª¤2å†…å®¹å­—æ•°: {len(all_results['step2_content'].split())}")
            print(f"æ­¥éª¤3ä¼˜åŒ–å­—æ•°: {len(all_results['step3_optimized'].split())}")

            # æ‰“å°è¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡
            print("\nğŸ“Š æ€»ä½“Tokenä½¿ç”¨ç»Ÿè®¡:")
            token_usage = all_results['metadata']['token_usage']
            total_input_tokens = 0
            total_output_tokens = 0
            total_tokens = 0
            
            for step_name, step_usage in token_usage.items():
                print(f"\n  {step_name}:")
                for model, usage in step_usage.items():
                    input_tokens = usage.get('input_tokens', 0)
                    output_tokens = usage.get('output_tokens', 0)
                    step_total = usage.get('total_tokens', 0)
                    
                    print(f"    æ¨¡å‹: {model}")
                    print(f"    è¾“å…¥: {input_tokens} tokens")
                    print(f"    è¾“å‡º: {output_tokens} tokens")
                    print(f"    å°è®¡: {step_total} tokens")
                    
                    total_input_tokens += input_tokens
                    total_output_tokens += output_tokens
                    total_tokens += step_total
            
            print(f"\nğŸ¯ å…¨æµç¨‹æ±‡æ€»:")
            print(f"  æ€»è¾“å…¥tokens: {total_input_tokens}")
            print(f"  æ€»è¾“å‡ºtokens: {total_output_tokens}")
            print(f"  æ€»è®¡tokens: {total_tokens}")

            # ä½ å¯ä»¥è®¿é—®ä»»ä½•ä¸­é—´ç»“æœå’Œtokenä¿¡æ¯
            outline = all_results["step1_outline"]
            content = all_results["step2_content"]
            optimized = all_results["step3_optimized"]
            token_stats = all_results["metadata"]["token_usage"]

            # éªŒè¯æ‰€æœ‰æ•°æ®éƒ½å­˜åœ¨
            self.assertIn("topic", all_results)
            self.assertIn("step1_outline", all_results)
            self.assertIn("step2_content", all_results)
            self.assertIn("step3_optimized", all_results)
            self.assertIn("token_usage", all_results["metadata"])

            print("\nâœ… è¯¦ç»†ç‰ˆå†…å®¹ç”Ÿæˆç®¡é“ï¼ˆå«Tokenè¿½è¸ªï¼‰æµ‹è¯•é€šè¿‡!")

        except Exception as e:
            print(f"âŒ è¯¦ç»†ç‰ˆå†…å®¹ç”Ÿæˆç®¡é“æµ‹è¯•å¤±è´¥: {e}")

    def test_content_generation_with_token_tracking_v2(self) -> None:
        """
        æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ - ä½¿ç”¨context manageræ–¹å¼è¿½è¸ªtoken
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ï¼ˆContext Manager Tokenè¿½è¸ªï¼‰ ===")
        
        try:
            from langchain_core.callbacks import get_usage_metadata_callback
            
            # ä¸»é¢˜æ‰©å±•
            topic_expander = ChatPromptTemplate.from_template(
                "ç»™å®šä¸»é¢˜ï¼š{topic}\nè¯·æ‰©å±•æˆä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†ã€‚"
            )

            # å†…å®¹ç”Ÿæˆ
            content_generator = ChatPromptTemplate.from_template(
                "åŸºäºä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ç¯‡ç»“æ„å®Œæ•´çš„æ–‡ç« ï¼š\n{step1_outline}\n\n"
                "è¦æ±‚ï¼š\n1. æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹\n2. è¯­è¨€æµç•…è‡ªç„¶\n3. é€»è¾‘æ¸…æ™°"
            )

            # å†…å®¹ä¼˜åŒ–
            content_optimizer = ChatPromptTemplate.from_template(
                "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{step2_content}\n\n"
                "ä¼˜åŒ–è¦æ±‚ï¼š\n1. å¢åŠ å…·ä½“ä¾‹å­\n2. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„æè¿°\n3. ä¿æŒåŸæœ‰ç»“æ„"
            )

            # ä½¿ç”¨context managerè¿½è¸ªæ‰€æœ‰tokenä½¿ç”¨
            with get_usage_metadata_callback() as cb:
                # æ„å»ºç®€åŒ–çš„ç®¡é“
                simple_pipeline = (
                    RunnablePassthrough.assign(
                        step1_outline=topic_expander | self.creative_model | self.str_parser
                    )
                    | RunnablePassthrough.assign(
                        step2_content=content_generator | self.creative_model | self.str_parser
                    )
                    | RunnablePassthrough.assign(
                        step3_optimized=content_optimizer | self.creative_model | self.str_parser
                    )
                )
                
                # æ‰§è¡Œç®¡é“ï¼Œæ‰€æœ‰tokenä½¿ç”¨éƒ½ä¼šè¢«è‡ªåŠ¨è¿½è¸ª
                test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"
                results = simple_pipeline.invoke({"topic": test_topic})
                
                # è·å–æ€»çš„tokenä½¿ç”¨æƒ…å†µ
                total_usage = cb.usage_metadata
            
            # æ˜¾ç¤ºç»“æœ
            print("\n=== å¤„ç†ç»“æœ ===")
            print(f"åŸå§‹ä¸»é¢˜: {results['topic']}")
            print(f"æ­¥éª¤1å¤§çº²å­—æ•°: {len(results['step1_outline'].split())}")
            print(f"æ­¥éª¤2å†…å®¹å­—æ•°: {len(results['step2_content'].split())}")
            print(f"æ­¥éª¤3ä¼˜åŒ–å­—æ•°: {len(results['step3_optimized'].split())}")
            
            # æ˜¾ç¤ºè¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡
            print("\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
            total_input = 0
            total_output = 0
            total_all = 0
            
            for model_name, usage_data in total_usage.items():
                input_tokens = usage_data.get('input_tokens', 0)
                output_tokens = usage_data.get('output_tokens', 0)
                total_tokens = usage_data.get('total_tokens', 0)
                
                print(f"\næ¨¡å‹: {model_name}")
                print(f"  è¾“å…¥tokens: {input_tokens}")
                print(f"  è¾“å‡ºtokens: {output_tokens}")
                print(f"  æ€»tokens: {total_tokens}")
                
                # å¦‚æœæœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
                if 'input_token_details' in usage_data:
                    print(f"  è¾“å…¥è¯¦æƒ…: {usage_data['input_token_details']}")
                if 'output_token_details' in usage_data:
                    print(f"  è¾“å‡ºè¯¦æƒ…: {usage_data['output_token_details']}")
                
                total_input += input_tokens
                total_output += output_tokens  
                total_all += total_tokens
            
            print(f"\nğŸ¯ æ•´ä¸ªç®¡é“æ±‡æ€»:")
            print(f"  æ€»è¾“å…¥tokens: {total_input}")
            print(f"  æ€»è¾“å‡ºtokens: {total_output}")
            print(f"  æ€»è®¡tokens: {total_all}")
            
            # éªŒè¯æ•°æ®
            self.assertIn("topic", results)
            self.assertIn("step1_outline", results)
            self.assertIn("step2_content", results)
            self.assertIn("step3_optimized", results)
            self.assertGreater(total_all, 0, "åº”è¯¥æœ‰tokenä½¿ç”¨è®°å½•")
            
            print("\nâœ… Context Manager Tokenè¿½è¸ªæµ‹è¯•é€šè¿‡!")
            
            # è¿”å›è¯¦ç»†ç»“æœä¾›è¿›ä¸€æ­¥åˆ†æ
            return {
                "results": results,
                "token_usage": total_usage,
                "summary": {
                    "total_input_tokens": total_input,
                    "total_output_tokens": total_output,
                    "total_tokens": total_all
                }
            }
            
        except Exception as e:
            print(f"âŒ Context Manager Tokenè¿½è¸ªæµ‹è¯•å¤±è´¥: {e}")
            raise

    def test_content_generation_step_by_step_tokens(self) -> None:
        """
        æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ - åˆ†æ­¥å®æ—¶è¿½è¸ªæ¯ä¸ªæ­¥éª¤çš„tokenä½¿ç”¨
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å†…å®¹ç”Ÿæˆç®¡é“ï¼ˆåˆ†æ­¥å®æ—¶Tokenè¿½è¸ªï¼‰ ===")
        
        try:
            from langchain_core.callbacks import get_usage_metadata_callback
            
            # ä¸»é¢˜æ‰©å±•
            topic_expander = ChatPromptTemplate.from_template(
                "ç»™å®šä¸»é¢˜ï¼š{topic}\nè¯·æ‰©å±•æˆä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼ŒåŒ…å«3-5ä¸ªä¸»è¦éƒ¨åˆ†ã€‚"
            )

            # å†…å®¹ç”Ÿæˆ
            content_generator = ChatPromptTemplate.from_template(
                "åŸºäºä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ç¯‡ç»“æ„å®Œæ•´çš„æ–‡ç« ï¼š\n{outline}\n\n"
                "è¦æ±‚ï¼š\n1. æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹\n2. è¯­è¨€æµç•…è‡ªç„¶\n3. é€»è¾‘æ¸…æ™°"
            )

            # å†…å®¹ä¼˜åŒ–
            content_optimizer = ChatPromptTemplate.from_template(
                "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡ç« ï¼Œä½¿å…¶æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ï¼š\n{content}\n\n"
                "ä¼˜åŒ–è¦æ±‚ï¼š\n1. å¢åŠ å…·ä½“ä¾‹å­\n2. ä½¿ç”¨æ›´ç”ŸåŠ¨çš„æè¿°\n3. ä¿æŒåŸæœ‰ç»“æ„"
            )
            
            # åˆ›å»ºå•ç‹¬çš„é“¾
            outline_chain = topic_expander | self.creative_model | self.str_parser
            content_chain = content_generator | self.creative_model | self.str_parser
            optimize_chain = content_optimizer | self.creative_model | self.str_parser
            
            # å­˜å‚¨æ¯æ­¥ç»“æœå’Œtokenä½¿ç”¨æƒ…å†µ
            step_results = {}
            step_tokens = {}
            
            test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨"
            
            # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²
            print("\nğŸš€ æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜å¤§çº²...")
            with get_usage_metadata_callback() as cb1:
                outline = outline_chain.invoke({"topic": test_topic})
                step_results["step1_outline"] = outline
                step_tokens["step1"] = dict(cb1.usage_metadata)
                
                print(f"âœ… å¤§çº²ç”Ÿæˆå®Œæˆ ({len(outline.split())} è¯)")
                if cb1.usage_metadata:
                    for model, usage in cb1.usage_metadata.items():
                        print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                              f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                              f"æ€»è®¡: {usage.get('total_tokens', 0)}")
            
            # æ­¥éª¤2ï¼šç”Ÿæˆå†…å®¹
            print("\nğŸš€ æ­¥éª¤2: åŸºäºå¤§çº²ç”Ÿæˆæ–‡ç« å†…å®¹...")
            with get_usage_metadata_callback() as cb2:
                content = content_chain.invoke({"outline": outline})
                step_results["step2_content"] = content
                step_tokens["step2"] = dict(cb2.usage_metadata)
                
                print(f"âœ… æ–‡ç« å†…å®¹ç”Ÿæˆå®Œæˆ ({len(content.split())} è¯)")
                if cb2.usage_metadata:
                    for model, usage in cb2.usage_metadata.items():
                        print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                              f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                              f"æ€»è®¡: {usage.get('total_tokens', 0)}")
            
            # æ­¥éª¤3ï¼šä¼˜åŒ–å†…å®¹
            print("\nğŸš€ æ­¥éª¤3: ä¼˜åŒ–æ–‡ç« å†…å®¹...")
            with get_usage_metadata_callback() as cb3:
                optimized_content = optimize_chain.invoke({"content": content})
                step_results["step3_optimized"] = optimized_content
                step_tokens["step3"] = dict(cb3.usage_metadata)
                
                print(f"âœ… å†…å®¹ä¼˜åŒ–å®Œæˆ ({len(optimized_content.split())} è¯)")
                if cb3.usage_metadata:
                    for model, usage in cb3.usage_metadata.items():
                        print(f"   Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens', 0)}, "
                              f"è¾“å‡º: {usage.get('output_tokens', 0)}, "
                              f"æ€»è®¡: {usage.get('total_tokens', 0)}")
            
            # æ±‡æ€»ç»Ÿè®¡
            print("\nğŸ“Š å®Œæ•´Tokenä½¿ç”¨åˆ†æ:")
            print("=" * 50)
            
            total_input_tokens = 0
            total_output_tokens = 0
            total_tokens = 0
            
            step_names = {
                "step1": "ä¸»é¢˜æ‰©å±•ä¸ºå¤§çº²",
                "step2": "å¤§çº²ç”Ÿæˆæ–‡ç« ", 
                "step3": "æ–‡ç« å†…å®¹ä¼˜åŒ–"
            }
            
            for step_id, step_name in step_names.items():
                print(f"\nğŸ“ {step_name}:")
                if step_id in step_tokens:
                    for model, usage in step_tokens[step_id].items():
                        input_t = usage.get('input_tokens', 0)
                        output_t = usage.get('output_tokens', 0)
                        total_t = usage.get('total_tokens', 0)
                        
                        print(f"   æ¨¡å‹: {model}")
                        print(f"   è¾“å…¥tokens: {input_t}")
                        print(f"   è¾“å‡ºtokens: {output_t}")
                        print(f"   æ­¥éª¤æ€»è®¡: {total_t}")
                        
                        total_input_tokens += input_t
                        total_output_tokens += output_t
                        total_tokens += total_t
                else:
                    print("   æ— tokenä½¿ç”¨æ•°æ®")
            
            print(f"\nğŸ¯ å…¨æµç¨‹æ±‡æ€»:")
            print(f"   æ€»è¾“å…¥tokens: {total_input_tokens}")
            print(f"   æ€»è¾“å‡ºtokens: {total_output_tokens}")
            print(f"   æµç¨‹æ€»è®¡tokens: {total_tokens}")
            
            # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
            if total_input_tokens > 0:
                efficiency_ratio = total_output_tokens / total_input_tokens
                print(f"   è¾“å‡º/è¾“å…¥æ¯”ç‡: {efficiency_ratio:.2f}")
            
            # å†…å®¹ç»Ÿè®¡
            print(f"\nğŸ“„ å†…å®¹ç»Ÿè®¡:")
            print(f"   åŸå§‹ä¸»é¢˜: {test_topic}")
            print(f"   å¤§çº²å­—æ•°: {len(step_results['step1_outline'].split())} è¯")
            print(f"   æ–‡ç« å­—æ•°: {len(step_results['step2_content'].split())} è¯")
            print(f"   ä¼˜åŒ–åå­—æ•°: {len(step_results['step3_optimized'].split())} è¯")
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            self.assertIn("step1_outline", step_results)
            self.assertIn("step2_content", step_results)
            self.assertIn("step3_optimized", step_results)
            self.assertGreater(total_tokens, 0, "åº”è¯¥æœ‰tokenä½¿ç”¨è®°å½•")
            
            print("\nâœ… åˆ†æ­¥å®æ—¶Tokenè¿½è¸ªæµ‹è¯•é€šè¿‡!")
            
            # è¿”å›å®Œæ•´çš„åˆ†æç»“æœ
            return {
                "topic": test_topic,
                "step_results": step_results,
                "step_tokens": step_tokens,
                "summary": {
                    "total_input_tokens": total_input_tokens,
                    "total_output_tokens": total_output_tokens,
                    "total_tokens": total_tokens,
                    "efficiency_ratio": total_output_tokens / total_input_tokens if total_input_tokens > 0 else 0
                }
            }
            
        except Exception as e:
            print(f"âŒ åˆ†æ­¥å®æ—¶Tokenè¿½è¸ªæµ‹è¯•å¤±è´¥: {e}")
            raise

    def test_async_batch_applications(self) -> None:
        """
        æµ‹è¯•å¼‚æ­¥æ‰¹å¤„ç†åº”ç”¨åœºæ™¯
        
        è¾“å…¥: æ— 
        è¾“å‡º: æ— 
        """
        print("\n=== æµ‹è¯•å¼‚æ­¥æ‰¹å¤„ç†åº”ç”¨ ===")
        
        async def run_async_test():
            try:
                # ç®€å•çš„ç¿»è¯‘é“¾
                translation_prompt = ChatPromptTemplate.from_template(
                    "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}"
                )
                
                translation_chain = translation_prompt | self.model | self.str_parser
                
                # æ‰¹é‡ç¿»è¯‘ä»»åŠ¡
                texts_to_translate = [
                    {"text": "ä½ å¥½ï¼Œä¸–ç•Œ"},
                    {"text": "äººå·¥æ™ºèƒ½æŠ€æœ¯"},
                    {"text": "æœºå™¨å­¦ä¹ ç®—æ³•"},
                    {"text": "è‡ªç„¶è¯­è¨€å¤„ç†"},
                    {"text": "æ·±åº¦å­¦ä¹ æ¨¡å‹"}
                ]
                
                # å¼‚æ­¥æ‰¹å¤„ç†
                results = await translation_chain.abatch(texts_to_translate)
                
                print("æ‰¹é‡ç¿»è¯‘ç»“æœ:")
                for i, (original, translated) in enumerate(zip(texts_to_translate, results)):
                    print(f"{i+1}. {original['text']} â†’ {translated}")
                
                self.assertEqual(len(results), len(texts_to_translate))
                for result in results:
                    self.assertIsInstance(result, str)
                    self.assertGreater(len(result), 0)
                
                print("âœ… å¼‚æ­¥æ‰¹å¤„ç†åº”ç”¨æµ‹è¯•é€šè¿‡!")
                
            except Exception as e:
                print(f"âŒ å¼‚æ­¥æ‰¹å¤„ç†åº”ç”¨æµ‹è¯•å¤±è´¥: {e}")
        
        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        asyncio.run(run_async_test())


if __name__ == "__main__":
    unittest.main() 