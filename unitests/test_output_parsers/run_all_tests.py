#!/usr/bin/env python3
"""
è¾“å‡ºè§£æå™¨æµ‹è¯•è¿è¡Œå™¨

è¿è¡Œæ‰€æœ‰è¾“å‡ºè§£æå™¨æµ‹è¯•ï¼Œå¹¶æä¾›è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šå’Œç»Ÿè®¡ä¿¡æ¯
"""

import sys
import unittest
import time
from typing import List, Dict, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from unitests.test_output_parsers.test_basic_parsers import TestBasicOutputParsers
from unitests.test_output_parsers.test_pydantic_parsers import TestPydanticOutputParsers
from unitests.test_output_parsers.test_custom_parsers import TestCustomOutputParsers
from unitests.test_output_parsers.test_error_handling import TestOutputParserErrorHandling


class OutputParserTestRunner:
    """è¾“å‡ºè§£æå™¨æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_suites = {
            'basic': TestBasicOutputParsers,
            'pydantic': TestPydanticOutputParsers,
            'custom': TestCustomOutputParsers,
            'error_handling': TestOutputParserErrorHandling
        }
        self.results = {}
    
    def run_basic_tests(self) -> unittest.TestResult:
        """è¿è¡ŒåŸºç¡€è§£æå™¨æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ”§ è¿è¡ŒåŸºç¡€è¾“å‡ºè§£æå™¨æµ‹è¯•")
        print("="*60)
        
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromTestCase(TestBasicOutputParsers)
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        self.results['basic'] = {
            'result': result,
            'duration': end_time - start_time,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
        }
        
        return result
    
    def run_pydantic_tests(self) -> unittest.TestResult:
        """è¿è¡ŒPydanticè§£æå™¨æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ—ï¸ è¿è¡ŒPydanticè¾“å‡ºè§£æå™¨æµ‹è¯•")
        print("="*60)
        
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromTestCase(TestPydanticOutputParsers)
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        self.results['pydantic'] = {
            'result': result,
            'duration': end_time - start_time,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
        }
        
        return result
    
    def run_custom_tests(self) -> unittest.TestResult:
        """è¿è¡Œè‡ªå®šä¹‰è§£æå™¨æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ› ï¸ è¿è¡Œè‡ªå®šä¹‰è¾“å‡ºè§£æå™¨æµ‹è¯•")
        print("="*60)
        
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromTestCase(TestCustomOutputParsers)
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        self.results['custom'] = {
            'result': result,
            'duration': end_time - start_time,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
        }
        
        return result
    
    def run_error_handling_tests(self) -> unittest.TestResult:
        """è¿è¡Œé”™è¯¯å¤„ç†æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš¨ è¿è¡Œé”™è¯¯å¤„ç†å’Œé«˜çº§åŠŸèƒ½æµ‹è¯•")
        print("="*60)
        
        loader = unittest.TestLoader()
        suite = loader.loadTestsFromTestCase(TestOutputParserErrorHandling)
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        self.results['error_handling'] = {
            'result': result,
            'duration': end_time - start_time,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
        }
        
        return result
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "ğŸ¯" * 30)
        print("ğŸ¯ è¾“å‡ºè§£æå™¨æµ‹è¯•å¥—ä»¶ - å…¨é¢æµ‹è¯•å¼€å§‹")
        print("ğŸ¯" * 30)
        
        overall_start_time = time.time()
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        try:
            self.run_basic_tests()
        except Exception as e:
            print(f"âš ï¸ åŸºç¡€è§£æå™¨æµ‹è¯•å¼‚å¸¸: {e}")
        
        try:
            self.run_pydantic_tests()
        except Exception as e:
            print(f"âš ï¸ Pydanticè§£æå™¨æµ‹è¯•å¼‚å¸¸: {e}")
            
        try:
            self.run_custom_tests()
        except Exception as e:
            print(f"âš ï¸ è‡ªå®šä¹‰è§£æå™¨æµ‹è¯•å¼‚å¸¸: {e}")
            
        try:
            self.run_error_handling_tests()
        except Exception as e:
            print(f"âš ï¸ é”™è¯¯å¤„ç†æµ‹è¯•å¼‚å¸¸: {e}")
        
        overall_end_time = time.time()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        summary = self._generate_summary(overall_end_time - overall_start_time)
        self._print_summary(summary)
        
        return summary
    
    def run_specific_tests(self, test_types: List[str]) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šç±»å‹çš„æµ‹è¯•"""
        overall_start_time = time.time()
        
        for test_type in test_types:
            if test_type == 'basic':
                self.run_basic_tests()
            elif test_type == 'pydantic':
                self.run_pydantic_tests()
            elif test_type == 'custom':
                self.run_custom_tests()
            elif test_type == 'error_handling':
                self.run_error_handling_tests()
            else:
                print(f"âš ï¸ æœªçŸ¥çš„æµ‹è¯•ç±»å‹: {test_type}")
        
        overall_end_time = time.time()
        summary = self._generate_summary(overall_end_time - overall_start_time)
        self._print_summary(summary)
        
        return summary
    
    def _generate_summary(self, total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        total_tests = sum(data.get('tests_run', 0) for data in self.results.values())
        total_failures = sum(data.get('failures', 0) for data in self.results.values())
        total_errors = sum(data.get('errors', 0) for data in self.results.values())
        total_success = total_tests - total_failures - total_errors
        
        return {
            'total_duration': total_duration,
            'total_tests': total_tests,
            'total_success': total_success,
            'total_failures': total_failures,
            'total_errors': total_errors,
            'overall_success_rate': (total_success / total_tests * 100) if total_tests > 0 else 0,
            'module_results': self.results
        }
    
    def _print_summary(self, summary: Dict[str, Any]) -> None:
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "ğŸ“Š" * 30)
        print("ğŸ“Š è¾“å‡ºè§£æå™¨æµ‹è¯•å¥—ä»¶ - æµ‹è¯•æŠ¥å‘Š")
        print("ğŸ“Š" * 30)
        
        print(f"\nâ±ï¸ æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
        print(f"ğŸ§ª æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… æˆåŠŸ: {summary['total_success']}")
        print(f"âŒ å¤±è´¥: {summary['total_failures']}")
        print(f"ğŸ’¥ é”™è¯¯: {summary['total_errors']}")
        print(f"ğŸ“ˆ æ€»æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
        
        print("\nğŸ“‹ æ¨¡å—è¯¦ç»†ç»“æœ:")
        print("-" * 80)
        print(f"{'æ¨¡å—':<20} {'æµ‹è¯•æ•°':<10} {'æˆåŠŸ':<10} {'å¤±è´¥':<10} {'é”™è¯¯':<10} {'æˆåŠŸç‡':<10} {'è€—æ—¶':<10}")
        print("-" * 80)
        
        module_names = {
            'basic': 'åŸºç¡€è§£æå™¨',
            'pydantic': 'Pydanticè§£æå™¨', 
            'custom': 'è‡ªå®šä¹‰è§£æå™¨',
            'error_handling': 'é”™è¯¯å¤„ç†'
        }
        
        for module, data in summary['module_results'].items():
            module_name = module_names.get(module, module)
            tests_run = data.get('tests_run', 0)
            failures = data.get('failures', 0)
            errors = data.get('errors', 0)
            success = tests_run - failures - errors
            success_rate = data.get('success_rate', 0)
            duration = data.get('duration', 0)
            
            print(f"{module_name:<20} {tests_run:<10} {success:<10} {failures:<10} {errors:<10} {success_rate:<9.1f}% {duration:<9.2f}s")
        
        print("-" * 80)
        
        # æ€§èƒ½è¯„ä¼°
        print("\nâš¡ æ€§èƒ½è¯„ä¼°:")
        avg_time_per_test = summary['total_duration'] / summary['total_tests'] if summary['total_tests'] > 0 else 0
        print(f"   å¹³å‡æ¯æµ‹è¯•è€—æ—¶: {avg_time_per_test:.3f}ç§’")
        
        if avg_time_per_test < 0.5:
            print("   ğŸŸ¢ æ€§èƒ½ä¼˜ç§€")
        elif avg_time_per_test < 2.0:
            print("   ğŸŸ¡ æ€§èƒ½è‰¯å¥½")
        else:
            print("   ğŸ”´ æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        
        # å¯é æ€§è¯„ä¼°
        print("\nğŸ”’ å¯é æ€§è¯„ä¼°:")
        if summary['overall_success_rate'] >= 95:
            print("   ğŸŸ¢ å¯é æ€§ä¼˜ç§€")
        elif summary['overall_success_rate'] >= 85:
            print("   ğŸŸ¡ å¯é æ€§è‰¯å¥½")
        else:
            print("   ğŸ”´ å¯é æ€§éœ€è¦æ”¹è¿›")
        
        # æµ‹è¯•è¦†ç›–è¯„ä¼°
        print("\nğŸ“Š æµ‹è¯•è¦†ç›–è¯„ä¼°:")
        total_modules = len(self.test_suites)
        tested_modules = len([m for m in summary['module_results'] if summary['module_results'][m].get('tests_run', 0) > 0])
        coverage_rate = tested_modules / total_modules * 100
        
        print(f"   æ¨¡å—è¦†ç›–ç‡: {coverage_rate:.1f}% ({tested_modules}/{total_modules})")
        
        if coverage_rate >= 90:
            print("   ğŸŸ¢ è¦†ç›–ç‡ä¼˜ç§€")
        elif coverage_rate >= 75:
            print("   ğŸŸ¡ è¦†ç›–ç‡è‰¯å¥½")
        else:
            print("   ğŸ”´ è¦†ç›–ç‡éœ€è¦æå‡")
        
        # æ¨èå»ºè®®
        print("\nğŸ’¡ æ¨èå»ºè®®:")
        if summary['total_failures'] > 0:
            print("   - æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¼˜åŒ–æ¨¡å‹é…ç½®")
        if summary['total_errors'] > 0:
            print("   - æ£€æŸ¥é”™è¯¯çš„æµ‹è¯•ç”¨ä¾‹ï¼Œç¡®è®¤ç¯å¢ƒé…ç½®")
        if avg_time_per_test > 1.0:
            print("   - è€ƒè™‘ä¼˜åŒ–æµ‹è¯•æ€§èƒ½ï¼Œå‡å°‘LLMè°ƒç”¨")
        if summary['overall_success_rate'] < 90:
            print("   - æå‡æµ‹è¯•ç¨³å®šæ€§ï¼ŒåŠ å¼ºé”™è¯¯å¤„ç†")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ„Ÿè°¢ä½¿ç”¨è¾“å‡ºè§£æå™¨æµ‹è¯•å¥—ä»¶ï¼")
    
    def list_available_tests(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•"""
        print("ğŸ“‹ å¯ç”¨çš„æµ‹è¯•æ¨¡å—:")
        print("-" * 50)
        
        descriptions = {
            'basic': 'åŸºç¡€è¾“å‡ºè§£æå™¨æµ‹è¯• - StrOutputParser, JsonOutputParser, XMLOutputParser, YAMLOutputParser',
            'pydantic': 'Pydanticè¾“å‡ºè§£æå™¨æµ‹è¯• - PydanticOutputParser, PydanticToolsParser, å¤æ‚æ¨¡å‹è§£æ',
            'custom': 'è‡ªå®šä¹‰è¾“å‡ºè§£æå™¨æµ‹è¯• - åˆ—è¡¨è§£æå™¨, æ­£åˆ™è§£æå™¨, æ¡ä»¶è§£æå™¨, é“¾å¼è§£æå™¨',
            'error_handling': 'é”™è¯¯å¤„ç†å’Œé«˜çº§åŠŸèƒ½æµ‹è¯• - OutputFixingParser, RetryWithErrorOutputParser, å›é€€ç­–ç•¥'
        }
        
        for test_type, test_class in self.test_suites.items():
            desc = descriptions.get(test_type, "")
            print(f"ğŸ”¹ {test_type}: {test_class.__name__}")
            print(f"   {desc}")
            print()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¾“å‡ºè§£æå™¨æµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument(
        '--tests', 
        nargs='*', 
        choices=['basic', 'pydantic', 'custom', 'error_handling', 'all'],
        default=['all'],
        help="è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹ (é»˜è®¤: all)"
    )
    parser.add_argument(
        '--list', 
        action='store_true',
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•"
    )
    parser.add_argument(
        '--quiet', 
        action='store_true',
        help="é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæ‘˜è¦"
    )
    parser.add_argument(
        '--benchmark',
        action='store_true',
        help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"
    )
    
    args = parser.parse_args()
    
    runner = OutputParserTestRunner()
    
    if args.list:
        runner.list_available_tests()
        return 0
    
    if args.quiet:
        # é‡å®šå‘è¾“å‡ºåˆ°é™é»˜æ¨¡å¼
        import io
        old_stdout = sys.stdout
        sys.stdout = io.StringIO()
    
    try:
        if 'all' in args.tests:
            summary = runner.run_all_tests()
        else:
            summary = runner.run_specific_tests(args.tests)
        
        if args.quiet:
            # æ¢å¤è¾“å‡ºå¹¶åªæ˜¾ç¤ºæ‘˜è¦
            sys.stdout = old_stdout
            runner._print_summary(summary)
        
        if args.benchmark:
            print("\nğŸ“Š è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸“é—¨çš„æ€§èƒ½æµ‹è¯•
            
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if summary['total_failures'] == 0 and summary['total_errors'] == 0:
            return 0
        else:
            return 1
            
    except KeyboardInterrupt:
        if args.quiet:
            sys.stdout = old_stdout
        print("\n\nâŒ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        if args.quiet:
            sys.stdout = old_stdout
        print(f"\n\nğŸ’¥ æµ‹è¯•è¿è¡Œå™¨å‘ç”Ÿé”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 