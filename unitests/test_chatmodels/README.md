# ChatModels æµ‹è¯•å¥—ä»¶

è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ ChatOpenAI æ¨¡å‹æµ‹è¯•å¥—ä»¶ï¼Œä¸“æ³¨äºæµ‹è¯•åŸºäº `gpt-4o-mini` æ¨¡å‹çš„å„ç§åŠŸèƒ½å’Œåº”ç”¨åœºæ™¯ã€‚æœ¬æµ‹è¯•å¥—ä»¶æ¶µç›–äº†ä»åŸºç¡€èŠå¤©åˆ°é«˜çº§åŠŸèƒ½çš„å®Œæ•´æµ‹è¯•èŒƒå›´ï¼Œç¡®ä¿æ¨¡å‹åœ¨å„ç§å®é™…åº”ç”¨åœºæ™¯ä¸‹çš„å¯é æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install -r test_requirements.txt

# æˆ–ä½¿ç”¨ uv
uv pip install -r test_requirements.txt
```

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
pytest test_basic_chat.py
pytest test_tool_calling.py
pytest test_multimodal.py

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»
pytest test_basic_chat.py::TestBasicChat
pytest test_tool_calling.py::TestToolCalling
pytest test_multimodal.py::TestMultimodalFeatures

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–¹æ³•
pytest test_basic_chat.py::TestBasicChat::test_simple_chat
pytest test_tool_calling.py::TestToolCalling::test_complete_tool_calling_flow
pytest test_multimodal.py::TestMultimodalFeatures::test_image_from_url

# è¿è¡Œå¤šæ¨¡æ€ç›¸å…³çš„æ‰€æœ‰æµ‹è¯•
pytest test_multimodal.py -k "image or pdf or audio"

# è¿è¡Œå·¥å…·è°ƒç”¨ç›¸å…³çš„æ‰€æœ‰æµ‹è¯•
pytest test_tool_calling.py test_advanced_features.py test_multimodal.py -k "tool"
```

## ğŸ“‹ æµ‹è¯•åˆ†ç±»

### ğŸ”§ å•å…ƒæµ‹è¯•ï¼ˆUnit Testsï¼‰
ä½¿ç”¨ mock å“åº”ï¼Œä¸éœ€è¦çœŸå® APIï¼š

```bash
# è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•
pytest -m "unit"

# æˆ–è€…è¿è¡Œé™¤é›†æˆæµ‹è¯•å¤–çš„æ‰€æœ‰æµ‹è¯•
pytest -m "not integration"
```

### ğŸŒ é›†æˆæµ‹è¯•ï¼ˆIntegration Testsï¼‰
éœ€è¦çœŸå®çš„æœ¬åœ° API æœåŠ¡ï¼š

```bash
# è¿è¡Œé›†æˆæµ‹è¯•ï¼ˆéœ€è¦æœ¬åœ°APIæœåŠ¡ï¼‰
pytest -m "integration"
```

### â±ï¸ æ€§èƒ½æµ‹è¯•ï¼ˆSlow Testsï¼‰
è€—æ—¶è¾ƒé•¿çš„æµ‹è¯•ï¼š

```bash
# è·³è¿‡æ…¢é€Ÿæµ‹è¯•
pytest -m "not slow"

# åªè¿è¡Œæ…¢é€Ÿæµ‹è¯•
pytest -m "slow"
```

### ğŸ”„ å¼‚æ­¥æµ‹è¯•ï¼ˆAsync Testsï¼‰
å¼‚æ­¥åŠŸèƒ½æµ‹è¯•ï¼š

```bash
# è¿è¡Œå¼‚æ­¥æµ‹è¯•
pytest -m "async_test"
```

## ğŸ¯ åŠŸèƒ½æµ‹è¯•è¯¦è§£

### 1. åŸºç¡€èŠå¤©åŠŸèƒ½ (`test_basic_chat.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
åŸºç¡€èŠå¤©åŠŸèƒ½æ˜¯æ‰€æœ‰å¯¹è¯AIåº”ç”¨çš„æ ¸å¿ƒï¼Œä¸»è¦åŒ…æ‹¬ï¼š
- **æ¨¡å‹åˆå§‹åŒ–å’Œé…ç½®**: ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®é…ç½®å„ç§å‚æ•°ï¼ˆtemperatureã€max_tokensç­‰ï¼‰
- **ç®€å•å¯¹è¯**: å¤„ç†ç”¨æˆ·çš„åŸºç¡€è¯¢é—®ï¼Œæ˜¯æ‰€æœ‰èŠå¤©åº”ç”¨çš„åŸºæœ¬éœ€æ±‚
- **ç³»ç»Ÿæ¶ˆæ¯**: ä¸ºAIè®¾å®šè§’è‰²å’Œè¡Œä¸ºå‡†åˆ™ï¼Œå¸¸ç”¨äºå®¢æœæœºå™¨äººã€ä¸“ä¸šåŠ©æ‰‹ç­‰åœºæ™¯
- **å¤šè½®å¯¹è¯**: ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå®ç°è¿ç»­å¯¹è¯ä½“éªŒ
- **å‚æ•°è°ƒä¼˜**: é€šè¿‡temperatureç­‰å‚æ•°æ§åˆ¶æ¨¡å‹çš„åˆ›é€ æ€§å’Œç¡®å®šæ€§

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸ“ æ™ºèƒ½å®¢æœç³»ç»Ÿï¼šå¤„ç†ç”¨æˆ·å’¨è¯¢å’Œé—®é¢˜è§£ç­”
- ğŸ“ åœ¨çº¿æ•™è‚²åŠ©æ‰‹ï¼šå›ç­”å­¦ç”Ÿé—®é¢˜ï¼Œæä¾›å­¦ä¹ æŒ‡å¯¼
- ğŸ¥ åŒ»ç–—å’¨è¯¢åŠ©æ‰‹ï¼šæä¾›åŸºç¡€å¥åº·ä¿¡æ¯ï¼ˆéœ€ä¸“ä¸šå®¡æ ¸ï¼‰
- ğŸ›’ ç”µå•†è´­ç‰©åŠ©æ‰‹ï¼šäº§å“æ¨èå’Œè´­ä¹°æŒ‡å¯¼
- ğŸ’¼ ä¼ä¸šå†…éƒ¨åŠ©æ‰‹ï¼šå‘˜å·¥é—®é¢˜è§£ç­”å’Œä¿¡æ¯æŸ¥è¯¢

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
def test_model_initialization() -> None:
    """æµ‹è¯•æ¨¡å‹çš„åŸºç¡€é…ç½®æ˜¯å¦æ­£ç¡®"""

# ç®€å•å¯¹è¯æµ‹è¯•  
def test_simple_chat() -> None:
    """æµ‹è¯•åŸºç¡€çš„é—®ç­”åŠŸèƒ½"""

# ç³»ç»Ÿæ¶ˆæ¯æµ‹è¯•
def test_system_message_chat() -> None:
    """æµ‹è¯•è§’è‰²è®¾å®šå’Œè¡Œä¸ºæŒ‡å¯¼"""

# å¤šè½®å¯¹è¯æµ‹è¯•
def test_multi_turn_conversation() -> None:
    """æµ‹è¯•ä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›"""

# å‚æ•°é…ç½®æµ‹è¯•
def test_different_temperatures() -> None:
    """æµ‹è¯•ä¸åŒåˆ›é€ æ€§å‚æ•°çš„æ•ˆæœ"""

# æ‰¹å¤„ç†æµ‹è¯•
def test_batch_processing() -> None:
    """æµ‹è¯•åŒæ—¶å¤„ç†å¤šä¸ªå¯¹è¯è¯·æ±‚"""
```

### 2. å¼‚æ­¥æ“ä½œåŠŸèƒ½ (`test_async_operations.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
å¼‚æ­¥æ“ä½œæ˜¯æ„å»ºé«˜æ€§èƒ½ã€é«˜å¹¶å‘AIåº”ç”¨çš„å…³é”®æŠ€æœ¯ï¼š
- **å¼‚æ­¥è°ƒç”¨**: éé˜»å¡å¼APIè°ƒç”¨ï¼Œæå‡åº”ç”¨å“åº”æ€§
- **å¹¶å‘å¤„ç†**: åŒæ—¶å¤„ç†å¤šä¸ªç”¨æˆ·è¯·æ±‚ï¼Œæé«˜ç³»ç»Ÿååé‡
- **å¼‚æ­¥æµå¼è¾“å‡º**: å®æ—¶å“åº”ç”¨æˆ·ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ
- **æ€§èƒ½ä¼˜åŒ–**: é€šè¿‡å¹¶å‘å‡å°‘æ€»ä½“å“åº”æ—¶é—´
- **èµ„æºç®¡ç†**: åˆç†åˆ©ç”¨ç³»ç»Ÿèµ„æºï¼Œé¿å…é˜»å¡

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸŒ é«˜å¹¶å‘Webåº”ç”¨ï¼šåŒæ—¶æœåŠ¡æ•°åƒç”¨æˆ·
- ğŸ“± ç§»åŠ¨åº”ç”¨åç«¯ï¼šå¿«é€Ÿå“åº”ç§»åŠ¨ç«¯è¯·æ±‚
- ğŸ¤– èŠå¤©æœºå™¨äººå¹³å°ï¼šå¤„ç†å¤šä¸ªå¯¹è¯ä¼šè¯
- ğŸ“Š æ•°æ®åˆ†æå¹³å°ï¼šæ‰¹é‡å¤„ç†åˆ†æè¯·æ±‚
- ğŸ® æ¸¸æˆAIï¼šå®æ—¶å“åº”ç©å®¶æ“ä½œ

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# åŸºç¡€å¼‚æ­¥è°ƒç”¨
def test_basic_async_invoke() -> None:
    """æµ‹è¯•å¼‚æ­¥APIè°ƒç”¨åŠŸèƒ½"""

# å¼‚æ­¥æµå¼è¾“å‡º
def test_async_streaming() -> None:
    """æµ‹è¯•å¼‚æ­¥å®æ—¶å“åº”åŠŸèƒ½"""

# å¼‚æ­¥æ‰¹å¤„ç†
def test_async_batch_processing() -> None:
    """æµ‹è¯•å¼‚æ­¥æ‰¹é‡å¤„ç†èƒ½åŠ›"""

# å¹¶å‘è¯·æ±‚å¤„ç†
def test_async_concurrent_requests() -> None:
    """æµ‹è¯•åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚çš„èƒ½åŠ›"""

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
def test_async_performance_timing() -> None:
    """å¯¹æ¯”å¹¶å‘ä¸é¡ºåºæ‰§è¡Œçš„æ€§èƒ½å·®å¼‚"""

# æ“ä½œå–æ¶ˆæµ‹è¯•
def test_async_cancellation() -> None:
    """æµ‹è¯•å¼‚æ­¥æ“ä½œçš„ä¸­æ–­å’Œå–æ¶ˆ"""
```

### 3. æµå¼è¾“å‡ºåŠŸèƒ½ (`test_streaming.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
æµå¼è¾“å‡ºæä¾›å®æ—¶å“åº”ä½“éªŒï¼Œè®©ç”¨æˆ·çœ‹åˆ°AI"æ€è€ƒ"çš„è¿‡ç¨‹ï¼š
- **å®æ—¶è¾“å‡º**: é€æ­¥æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œå‡å°‘ç­‰å¾…æ„Ÿ
- **ç”¨æˆ·ä½“éªŒä¼˜åŒ–**: æ¨¡æ‹ŸçœŸå®å¯¹è¯çš„é€å­—æ˜¾ç¤ºæ•ˆæœ
- **é•¿å†…å®¹å¤„ç†**: å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œç”¨æˆ·å¯ä»¥æå‰å¼€å§‹é˜…è¯»
- **å“åº”å¼è®¾è®¡**: æ”¯æŒä¸­æ–­å’Œå®æ—¶åé¦ˆ
- **å›è°ƒå¤„ç†**: è‡ªå®šä¹‰è¾“å‡ºå¤„ç†é€»è¾‘

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸ’¬ å³æ—¶èŠå¤©åº”ç”¨ï¼šæ¨¡æ‹ŸçœŸäººèŠå¤©çš„æ‰“å­—æ•ˆæœ
- âœï¸ å†…å®¹åˆ›ä½œå·¥å…·ï¼šå®æ—¶æ˜¾ç¤ºæ–‡ç« ã€ä»£ç ç”Ÿæˆè¿‡ç¨‹
- ğŸ“ å†™ä½œåŠ©æ‰‹ï¼šè®©ç”¨æˆ·çœ‹åˆ°AIçš„åˆ›ä½œæ€è·¯
- ğŸ­ åˆ›æ„æ•…äº‹ç”Ÿæˆï¼šå¢å¼ºäº’åŠ¨æ€§å’Œå¨±ä¹æ€§
- ğŸ” å®æ—¶æœç´¢é—®ç­”ï¼šè¾¹æœç´¢è¾¹æ˜¾ç¤ºç»“æœ

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# åŸºç¡€æµå¼è¾“å‡º
def test_basic_streaming() -> None:
    """æµ‹è¯•åŸºæœ¬çš„æµå¼å“åº”åŠŸèƒ½"""

# ç³»ç»Ÿæ¶ˆæ¯æµå¼è¾“å‡º
def test_streaming_with_system_message() -> None:
    """æµ‹è¯•å¸¦è§’è‰²è®¾å®šçš„æµå¼è¾“å‡º"""

# å›è°ƒå¤„ç†å™¨
def test_streaming_callback_handler() -> None:
    """æµ‹è¯•è‡ªå®šä¹‰æµå¼è¾“å‡ºå¤„ç†é€»è¾‘"""

# æ•°æ®æ ¼å¼éªŒè¯
def test_streaming_chunk_format() -> None:
    """éªŒè¯æµå¼è¾“å‡ºæ•°æ®çš„æ­£ç¡®æ ¼å¼"""

# é•¿å†…å®¹å¤„ç†
def test_streaming_long_response() -> None:
    """æµ‹è¯•é•¿æ–‡æœ¬çš„æµå¼è¾“å‡ºæ•ˆæœ"""

# åŠŸèƒ½å¯¹æ¯”æµ‹è¯•
def test_streaming_vs_normal() -> None:
    """å¯¹æ¯”æµå¼ä¸æ™®é€šè¾“å‡ºçš„å·®å¼‚"""
```

### 4. é”™è¯¯å¤„ç†åŠŸèƒ½ (`test_error_handling.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
å®Œå–„çš„é”™è¯¯å¤„ç†ç¡®ä¿åº”ç”¨åœ¨å„ç§å¼‚å¸¸æƒ…å†µä¸‹çš„ç¨³å®šæ€§ï¼š
- **ç½‘ç»œå¼‚å¸¸å¤„ç†**: åº”å¯¹ç½‘ç»œä¸ç¨³å®šã€è¿æ¥ä¸­æ–­ç­‰æƒ…å†µ
- **APIé”™è¯¯å¤„ç†**: å¤„ç†å¯†é’¥é”™è¯¯ã€æƒé™é—®é¢˜ç­‰APIç›¸å…³é”™è¯¯
- **è¶…æ—¶ç®¡ç†**: åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
- **èµ„æºé™åˆ¶**: å¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™ã€é€Ÿç‡é™åˆ¶ç­‰é—®é¢˜
- **ä¼˜é›…é™çº§**: åœ¨å‡ºé”™æ—¶æä¾›å‹å¥½çš„ç”¨æˆ·ä½“éªŒ

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸŒ å…¨çƒåŒ–åº”ç”¨ï¼šåº”å¯¹ä¸åŒåœ°åŒºçš„ç½‘ç»œç¯å¢ƒå·®å¼‚
- ğŸ“± ç§»åŠ¨åº”ç”¨ï¼šå¤„ç†ç½‘ç»œåˆ‡æ¢ã€ä¿¡å·ä¸ç¨³å®šæƒ…å†µ
- ğŸ¢ ä¼ä¸šçº§åº”ç”¨ï¼šç¡®ä¿é«˜å¯ç”¨æ€§å’Œç¨³å®šæ€§
- â˜ï¸ äº‘æœåŠ¡é›†æˆï¼šå¤„ç†å„ç§äº‘æœåŠ¡çš„å¼‚å¸¸æƒ…å†µ
- ğŸ”’ å®‰å…¨æ•æ„Ÿåº”ç”¨ï¼šå¦¥å–„å¤„ç†æƒé™å’Œè®¤è¯é”™è¯¯

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# ç½‘ç»œè¿æ¥é”™è¯¯
def test_connection_error() -> None:
    """æµ‹è¯•ç½‘ç»œè¿æ¥å¤±è´¥çš„å¤„ç†"""

# è®¤è¯é”™è¯¯å¤„ç†
def test_invalid_api_key_error() -> None:
    """æµ‹è¯•APIå¯†é’¥é”™è¯¯çš„å¤„ç†"""

# è¶…æ—¶é”™è¯¯å¤„ç†
def test_timeout_error() -> None:
    """æµ‹è¯•è¯·æ±‚è¶…æ—¶çš„å¤„ç†æœºåˆ¶"""

# èµ„æºé™åˆ¶é”™è¯¯
def test_context_length_exceeded_error() -> None:
    """æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™çš„å¤„ç†"""

# æµå¼ä¸­æ–­å¤„ç†
def test_streaming_interruption_error() -> None:
    """æµ‹è¯•æµå¼è¾“å‡ºä¸­æ–­çš„æ¢å¤"""

# ç³»ç»ŸéŸ§æ€§æµ‹è¯•
def test_network_resilience() -> None:
    """æµ‹è¯•ç³»ç»Ÿçš„ç½‘ç»œæ¢å¤èƒ½åŠ›"""

# é”™è¯¯æ¢å¤æµ‹è¯•
def test_error_recovery() -> None:
    """æµ‹è¯•ä»é”™è¯¯çŠ¶æ€æ¢å¤çš„èƒ½åŠ›"""
```

### 5. é«˜çº§åŠŸèƒ½ (`test_advanced_features.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
é«˜çº§åŠŸèƒ½æ”¯æŒå¤æ‚çš„AIåº”ç”¨å¼€å‘éœ€æ±‚ï¼š
- **ç»“æ„åŒ–è¾“å‡º**: è¿”å›æ ¼å¼åŒ–æ•°æ®ï¼Œä¾¿äºç¨‹åºå¤„ç†å’Œé›†æˆ
- **ä¸Šä¸‹æ–‡ç®¡ç†**: æ™ºèƒ½ç®¡ç†é•¿å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **æ‰¹é‡å¤„ç†**: é«˜æ•ˆå¤„ç†å¤§é‡è¯·æ±‚
- **åˆ›é€ æ€§æ§åˆ¶**: é€šè¿‡å‚æ•°è°ƒèŠ‚æ¨¡å‹çš„åˆ›é€ æ€§å’Œç¡®å®šæ€§

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸ“Š æ•°æ®åˆ†æï¼šè¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœ
- ğŸ“ å†…å®¹ç”Ÿæˆï¼šåˆ›é€ æ€§å†™ä½œå’Œå†…å®¹åˆ›ä½œ
- ğŸ¢ ä¼ä¸šåº”ç”¨ï¼šæ‰¹é‡å¤„ç†ä¸šåŠ¡è¯·æ±‚
- ğŸ¯ å†³ç­–æ”¯æŒï¼šç»¼åˆå¤šä¸ªæ•°æ®æºè¿›è¡Œåˆ†æå†³ç­–

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# ç»“æ„åŒ–è¾“å‡ºè§£æ
def test_pydantic_tools_parser() -> None:
    """æµ‹è¯•ç»“æ„åŒ–æ•°æ®çš„è§£æå¤„ç†"""

# æ ¼å¼åŒ–è¾“å‡º
def test_structured_output_with_pydantic() -> None:
    """æµ‹è¯•è¿”å›ç‰¹å®šæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®"""

# ä¸Šä¸‹æ–‡ä¿æŒ
def test_multi_turn_conversation_with_context() -> None:
    """æµ‹è¯•å¤æ‚å¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†"""

# æ‰¹é‡å¤„ç†
def test_batch_processing() -> None:
    """æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½"""

# åˆ›é€ æ€§æ§åˆ¶
def test_temperature_and_creativity_effects() -> None:
    """æµ‹è¯•å‚æ•°å¯¹AIåˆ›é€ æ€§çš„å½±å“"""
```

### 6. å¤šæ¨¡æ€åŠŸèƒ½ (`test_multimodal.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
å¤šæ¨¡æ€åŠŸèƒ½è®©AIèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ä¹‹å¤–çš„å¤šç§æ•°æ®ç±»å‹ï¼Œå®ç°çœŸæ­£çš„å¤šæ¨¡æ€ç†è§£ï¼š
- **å›¾åƒå¤„ç†**: åˆ†æå’Œæè¿°å›¾ç‰‡å†…å®¹ï¼Œæ”¯æŒbase64å’ŒURLä¸¤ç§è¾“å…¥æ–¹å¼
- **PDFæ–‡æ¡£å¤„ç†**: è§£æå’Œç†è§£PDFæ–‡æ¡£å†…å®¹
- **éŸ³é¢‘å¤„ç†**: åˆ†æéŸ³é¢‘æ–‡ä»¶å†…å®¹å’Œç‰¹å¾
- **å¤šæ¨¡æ€å·¥å…·è°ƒç”¨**: ç»“åˆå¤šæ¨¡æ€è¾“å…¥ä¸å·¥å…·è°ƒç”¨åŠŸèƒ½
- **è·¨æ¨¡æ€ç†è§£**: ç»¼åˆå¤šç§æ•°æ®ç±»å‹è¿›è¡Œåˆ†æå’Œå†³ç­–

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸ–¼ï¸ å›¾åƒåˆ†æç³»ç»Ÿï¼šåŒ»ç–—å½±åƒè¯Šæ–­ã€äº§å“è´¨é‡æ£€æµ‹ã€å†…å®¹å®¡æ ¸
- ğŸ“„ æ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼šåˆåŒåˆ†æã€æŠ¥å‘Šæ€»ç»“ã€è¡¨æ ¼æ•°æ®æå–
- ğŸµ éŸ³é¢‘åˆ†æç³»ç»Ÿï¼šè¯­éŸ³è½¬æ–‡å­—ã€éŸ³ä¹åˆ†æã€å£°éŸ³è¯†åˆ«
- ğŸ¤– æ™ºèƒ½åŠ©æ‰‹ï¼šå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„å„ç§æ–‡ä»¶ç±»å‹
- ğŸ¯ å†³ç­–æ”¯æŒï¼šç»¼åˆæ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£ç­‰å¤šç»´åº¦ä¿¡æ¯

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# å›¾åƒå¤„ç†æµ‹è¯•
def test_image_from_base64_data() -> None:
    """æµ‹è¯•base64ç¼–ç å›¾åƒçš„å¤„ç†èƒ½åŠ›"""

def test_image_from_url() -> None:
    """æµ‹è¯•URLå›¾åƒçš„å¤„ç†èƒ½åŠ›"""

def test_multiple_images_comparison() -> None:
    """æµ‹è¯•å¤šå›¾åƒå¯¹æ¯”åˆ†æ"""

# PDFæ–‡æ¡£å¤„ç†æµ‹è¯•
def test_pdf_from_base64_data() -> None:
    """æµ‹è¯•PDFæ–‡æ¡£å†…å®¹è§£æ"""

# éŸ³é¢‘å¤„ç†æµ‹è¯•
def test_audio_from_base64_data() -> None:
    """æµ‹è¯•éŸ³é¢‘æ–‡ä»¶åˆ†æ"""

# å¤šæ¨¡æ€å·¥å…·è°ƒç”¨æµ‹è¯•
def test_multimodal_with_tool_calling() -> None:
    """æµ‹è¯•å¤šæ¨¡æ€è¾“å…¥ç»“åˆå·¥å…·è°ƒç”¨"""

# é”™è¯¯å¤„ç†æµ‹è¯•
def test_invalid_image_url() -> None:
    """æµ‹è¯•æ— æ•ˆå›¾åƒURLçš„é”™è¯¯å¤„ç†"""

def test_unsupported_file_type() -> None:
    """æµ‹è¯•ä¸æ”¯æŒæ–‡ä»¶ç±»å‹çš„é”™è¯¯å¤„ç†"""
```

### 7. å·¥å…·è°ƒç”¨åŠŸèƒ½ (`test_tool_calling.py`)

#### åŠŸèƒ½ä½œç”¨ä¸åº”ç”¨åœºæ™¯
å·¥å…·è°ƒç”¨æ˜¯è®©AIèƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å‡½æ•°å’ŒAPIçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæå¤§æ‰©å±•äº†AIçš„èƒ½åŠ›è¾¹ç•Œï¼š
- **å·¥å…·ç»‘å®š**: å°†å¤–éƒ¨å‡½æ•°ç»‘å®šåˆ°AIæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè°ƒç”¨
- **å•å·¥å…·è°ƒç”¨**: è°ƒç”¨å•ä¸ªå·¥å…·å®Œæˆç‰¹å®šä»»åŠ¡
- **å¤šå·¥å…·ååŒ**: ç»„åˆå¤šä¸ªå·¥å…·å®Œæˆå¤æ‚ä»»åŠ¡
- **å¹¶è¡Œå·¥å…·è°ƒç”¨**: åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·æé«˜æ•ˆç‡
- **å·¥å…·é“¾æ‰§è¡Œ**: ä¸€ä¸ªå·¥å…·çš„è¾“å‡ºä½œä¸ºå¦ä¸€ä¸ªå·¥å…·çš„è¾“å…¥
- **é”™è¯¯å¤„ç†**: å¦¥å–„å¤„ç†å·¥å…·æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§é”™è¯¯
- **å¼‚æ­¥å·¥å…·è°ƒç”¨**: æ”¯æŒå¼‚æ­¥æ‰§è¡Œå·¥å…·æé«˜æ€§èƒ½
- **æµå¼å·¥å…·è°ƒç”¨**: ç»“åˆæµå¼è¾“å‡ºå®æ—¶æ˜¾ç¤ºå·¥å…·æ‰§è¡Œè¿‡ç¨‹

**å®é™…åº”ç”¨åœºæ™¯**:
- ğŸ§  æ™ºèƒ½åŠ©æ‰‹ï¼šé›†æˆæ—¥å†ã€é‚®ä»¶ã€å¤©æ°”ã€æœç´¢ç­‰å¤šç§æœåŠ¡
- ğŸ› ï¸ å¼€å‘å·¥å…·ï¼šä»£ç ç”Ÿæˆã€æµ‹è¯•æ‰§è¡Œã€æ–‡æ¡£ç”Ÿæˆã€APIè°ƒç”¨
- ğŸª ç”µå•†å¹³å°ï¼šä»·æ ¼æŸ¥è¯¢ã€åº“å­˜æ£€æŸ¥ã€è®¢å•å¤„ç†ã€æ”¯ä»˜é›†æˆ
- ğŸ“Š æ•°æ®åˆ†æï¼šæ•°æ®åº“æŸ¥è¯¢ã€è®¡ç®—å·¥å…·ã€å›¾è¡¨ç”Ÿæˆ
- ğŸ¥ åŒ»ç–—ç³»ç»Ÿï¼šç—…å†æŸ¥è¯¢ã€è¯ç‰©ä¿¡æ¯ã€é¢„çº¦ç®¡ç†
- ğŸ¦ é‡‘èæœåŠ¡ï¼šè´¦æˆ·æŸ¥è¯¢ã€äº¤æ˜“æ‰§è¡Œã€é£é™©è®¡ç®—
- ğŸ® æ¸¸æˆAIï¼šæ¸¸æˆçŠ¶æ€æŸ¥è¯¢ã€åŠ¨ä½œæ‰§è¡Œã€è§„åˆ™æ£€æŸ¥

**ç›¸å…³æµ‹è¯•æ ·ä¾‹**:
```python
# ================== åŸºç¡€å·¥å…·è°ƒç”¨æµ‹è¯• ==================

# å•å·¥å…·ç»‘å®š
def test_single_tool_binding() -> None:
    """æµ‹è¯•å•ä¸ªå·¥å…·çš„ç»‘å®šåŠŸèƒ½"""

# å•å·¥å…·è°ƒç”¨
def test_single_tool_call() -> None:
    """æµ‹è¯•å•ä¸ªå·¥å…·çš„è°ƒç”¨å’Œå‚æ•°ä¼ é€’"""

# å¤šå·¥å…·ç»‘å®š
def test_multiple_tools_binding() -> None:
    """æµ‹è¯•å¤šä¸ªå·¥å…·çš„ç»‘å®šåŠŸèƒ½"""

# å¹¶è¡Œå·¥å…·è°ƒç”¨
def test_parallel_tool_calls() -> None:
    """æµ‹è¯•åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·"""

# ================== å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹æµ‹è¯• ==================

# å®Œæ•´æµç¨‹
def test_complete_tool_calling_flow() -> None:
    """æµ‹è¯•å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹ï¼šè¯·æ±‚->è°ƒç”¨->æ‰§è¡Œ->è¿”å›"""

# IDåŒ¹é…éªŒè¯
def test_tool_message_id_matching() -> None:
    """æµ‹è¯•ToolMessageçš„tool_call_idåŒ¹é…æœºåˆ¶"""

# ================== å¤æ‚å·¥å…·è°ƒç”¨æµ‹è¯• ==================

# å¤æ‚å‚æ•°å·¥å…·
def test_complex_tool_with_multiple_parameters() -> None:
    """æµ‹è¯•å…·æœ‰å¤šä¸ªå‚æ•°çš„å¤æ‚å·¥å…·"""

# å¯é€‰å‚æ•°å·¥å…·
def test_tool_with_optional_parameters() -> None:
    """æµ‹è¯•å…·æœ‰å¯é€‰å‚æ•°çš„å·¥å…·"""

# åµŒå¥—å·¥å…·è°ƒç”¨
def test_nested_tool_calls() -> None:
    """æµ‹è¯•å·¥å…·é“¾å¼è°ƒç”¨ï¼ˆä¸€ä¸ªå·¥å…·çš„ç»“æœç”¨äºå¦ä¸€ä¸ªå·¥å…·ï¼‰"""

# ================== å·¥å…·é”™è¯¯å¤„ç†æµ‹è¯• ==================

# æ‰§è¡Œé”™è¯¯å¤„ç†
def test_tool_execution_error_handling() -> None:
    """æµ‹è¯•å·¥å…·æ‰§è¡Œé”™è¯¯çš„å¤„ç†æœºåˆ¶"""

# æ— æ•ˆå‚æ•°å¤„ç†
def test_invalid_tool_parameters() -> None:
    """æµ‹è¯•æ— æ•ˆå·¥å…·å‚æ•°çš„å¤„ç†"""

# ================== å¼‚æ­¥å’Œæµå¼å·¥å…·è°ƒç”¨æµ‹è¯• ==================

# å¼‚æ­¥å·¥å…·è°ƒç”¨
def test_async_tool_calling() -> None:
    """æµ‹è¯•å¼‚æ­¥å·¥å…·è°ƒç”¨åŠŸèƒ½"""

# æµå¼å·¥å…·è°ƒç”¨
def test_streaming_with_tools() -> None:
    """æµ‹è¯•æµå¼è¾“å‡ºä¸å·¥å…·è°ƒç”¨çš„ç»“åˆ"""

# ================== ç»“æ„åŒ–å·¥å…·è¾“å‡ºæµ‹è¯• ==================

# ç»“æ„åŒ–è¾“å‡º
def test_structured_tool_output() -> None:
    """æµ‹è¯•å·¥å…·çš„ç»“æ„åŒ–è¾“å‡ºå¤„ç†"""

# ================== çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯• ==================

# å¤šæ­¥éª¤å·¥ä½œæµ
def test_multi_step_workflow() -> None:
    """æµ‹è¯•å¤šæ­¥éª¤å·¥ä½œæµï¼ˆè·å–ä¿¡æ¯->æ‰§è¡Œæ“ä½œï¼‰"""

# æ•°æ®åˆ†æåœºæ™¯
def test_data_analysis_scenario() -> None:
    """æµ‹è¯•æ•°æ®åˆ†æåœºæ™¯ï¼ˆè®¡ç®—->æœç´¢->è§£é‡Šï¼‰"""

# å†³ç­–æ”¯æŒåœºæ™¯
def test_decision_support_scenario() -> None:
    """æµ‹è¯•å†³ç­–æ”¯æŒåœºæ™¯ï¼ˆå¤šç»´åº¦ä¿¡æ¯æ”¶é›†ï¼‰"""

# ================== æ€§èƒ½å’Œé™åˆ¶æµ‹è¯• ==================

# å·¥å…·è°ƒç”¨é™åˆ¶
def test_tool_call_limits() -> None:
    """æµ‹è¯•å·¥å…·è°ƒç”¨æ•°é‡å’Œå¤æ‚åº¦é™åˆ¶"""

# æ€§èƒ½è®¡æ—¶
def test_tool_performance_timing() -> None:
    """æµ‹è¯•å·¥å…·è°ƒç”¨çš„æ€§èƒ½è¡¨ç°"""
```

## âš™ï¸ é…ç½®è¯´æ˜

### API é…ç½®

æµ‹è¯•ä½¿ç”¨ `src/config/api.py` ä¸­çš„ `local` é…ç½®ï¼š

```python
apis = {
    "local": {
        "base_url": "http://localhost:8212",
        "api_key": "sk-nsbaxS65nDJyGfA8wp5z7pbHxKUjEQBCpN5BKg7E19nLnOgL",
    }
}
```

### æ¨¡å‹é…ç½®

æ‰€æœ‰æµ‹è¯•ä½¿ç”¨ `gpt-4o-mini` æ¨¡å‹ï¼Œå…·ä½“å‚æ•°ï¼š

- **model**: `gpt-4o-mini`
- **temperature**: `0.7`ï¼ˆå¯é…ç½®ï¼‰
- **max_tokens**: `1000`ï¼ˆå¯é…ç½®ï¼‰
- **timeout**: `30`ç§’
- **max_retries**: `3`

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹æµ‹è¯•é…ç½®

ç¼–è¾‘ `conftest.py` ä¸­çš„ `test_config` fixtureï¼š

```python
@pytest.fixture(scope="session")
def test_config() -> dict:
    return {
        "model_name": "gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": 1000,
        "timeout": 30,
        "max_retries": 3
    }
```

### ä½¿ç”¨ä¸åŒçš„ API é…ç½®

åˆ›å»ºä¸´æ—¶é…ç½®ç”¨äºæµ‹è¯•ï¼š

```python
def test_with_custom_config():
    config = {
        "base_url": "http://alternative-server:8080",
        "api_key": "alternative-key"
    }
    model = ChatOpenAI(**config)
    # æµ‹è¯•...
```

## ğŸ› ï¸ å·¥å…·è°ƒç”¨ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€å·¥å…·è°ƒç”¨ç¤ºä¾‹

```python
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# å®šä¹‰å·¥å…·
@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers together."""
    return a + b

# åˆ›å»ºæ¨¡å‹å¹¶ç»‘å®šå·¥å…·
model = ChatOpenAI(model="gpt-4o-mini")
model_with_tools = model.bind_tools([add_numbers])

# è°ƒç”¨å·¥å…·
messages = [HumanMessage(content="What is 15 + 27?")]
response = model_with_tools.invoke(messages)

if response.tool_calls:
    # æ‰§è¡Œå·¥å…·
    for tool_call in response.tool_calls:
        tool_output = add_numbers.invoke(tool_call)
        print(f"Result: {tool_output.content}")
```

### å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹ç¤ºä¾‹

```python
# å®Œæ•´çš„å·¥å…·è°ƒç”¨å¯¹è¯æµç¨‹
def complete_tool_calling_example():
    messages = [HumanMessage(content="Calculate 8 * 7 and then add 15")]
    
    # ç¬¬ä¸€æ­¥ï¼šæ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·
    ai_response = model_with_tools.invoke(messages)
    messages.append(ai_response)
    
    # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå·¥å…·è°ƒç”¨
    for tool_call in ai_response.tool_calls:
        tool_output = selected_tool.invoke(tool_call)
        messages.append(tool_output)
    
    # ç¬¬ä¸‰æ­¥ï¼šè·å–æœ€ç»ˆç­”æ¡ˆ
    final_response = model_with_tools.invoke(messages)
    return final_response.content
```

### å¤šå·¥å…·ååŒç¤ºä¾‹

```python
from datetime import datetime

@tool
def get_weather(location: str) -> dict:
    """Get weather information for a location."""
    return {"location": location, "temperature": 22, "condition": "sunny"}

@tool
def calculate_travel_time(distance: float, speed: float) -> dict:
    """Calculate travel time given distance and speed."""
    time_hours = distance / speed
    return {"time_hours": time_hours, "time_minutes": time_hours * 60}

# ç»‘å®šå¤šä¸ªå·¥å…·
tools = [get_weather, calculate_travel_time, add_numbers]
model_with_tools = model.bind_tools(tools)

# å¤æ‚æŸ¥è¯¢ç¤ºä¾‹
query = "What's the weather in Beijing? Also, if I travel 120km at 60km/h, how long will it take?"
response = model_with_tools.invoke([HumanMessage(content=query)])

# æ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶è°ƒç”¨
```

## ğŸ“§ æ¶ˆæ¯æ ¼å¼è½¬æ¢

### LangChainæ¶ˆæ¯è½¬æ¢ä¸ºOpenAIæ ¼å¼

LangChainæä¾›äº† `convert_to_openai_messages` å‡½æ•°ï¼Œå¯ä»¥å°†LangChainçš„æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºOpenAI APIå…¼å®¹çš„æ ¼å¼ã€‚è¿™åœ¨ä¸å…¶ä»–OpenAIå…¼å®¹çš„APIæœåŠ¡é›†æˆæ—¶éå¸¸æœ‰ç”¨ã€‚

#### åŠŸèƒ½è¯´æ˜

- **æ¶ˆæ¯ç±»å‹æ”¯æŒ**: æ”¯æŒSystemMessageã€HumanMessageã€AIMessageã€ToolMessageç­‰æ‰€æœ‰LangChainæ¶ˆæ¯ç±»å‹
- **å†…å®¹æ ¼å¼å¤„ç†**: æ”¯æŒæ–‡æœ¬å†…å®¹å’Œå¤šæ¨¡æ€å†…å®¹ï¼ˆå›¾ç‰‡ã€æ–‡ä»¶ç­‰ï¼‰
- **å·¥å…·è°ƒç”¨è½¬æ¢**: è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨æ ¼å¼çš„è½¬æ¢
- **çµæ´»çš„æ–‡æœ¬æ ¼å¼**: æ”¯æŒå­—ç¬¦ä¸²å’Œå—æ ¼å¼çš„å†…å®¹å¤„ç†

#### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain_core.messages import (
    convert_to_openai_messages,
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)

# ç¤ºä¾‹1: åŸºç¡€æ¶ˆæ¯è½¬æ¢
messages = [
    SystemMessage("You are a helpful assistant."),
    HumanMessage("Hello, how are you?"),
    AIMessage("I'm doing well, thank you!")
]

# è½¬æ¢ä¸ºOpenAIæ ¼å¼
openai_messages = convert_to_openai_messages(messages)
print(openai_messages)
# è¾“å‡º:
# [
#   {'role': 'system', 'content': 'You are a helpful assistant.'},
#   {'role': 'user', 'content': 'Hello, how are you?'},
#   {'role': 'assistant', 'content': "I'm doing well, thank you!"}
# ]
```

#### å¤šæ¨¡æ€å†…å®¹è½¬æ¢ç¤ºä¾‹

```python
# ç¤ºä¾‹2: å¤šæ¨¡æ€å†…å®¹è½¬æ¢
multimodal_messages = [
    SystemMessage([{"type": "text", "text": "You are an image analysis assistant."}]),
    {
        "role": "user", 
        "content": [
            {"type": "text", "text": "What's in this image?"}, 
            {
                "type": "image_url", 
                "image_url": {"url": "data:image/png;base64,'/9j/4AAQSk'"}
            }
        ]
    },
    AIMessage("I can see a beautiful landscape in the image."),
]

openai_messages = convert_to_openai_messages(multimodal_messages)
print(openai_messages)
# è¾“å‡º:
# [
#   {'role': 'system', 'content': 'You are an image analysis assistant.'},
#   {
#     'role': 'user', 
#     'content': [
#       {'type': 'text', 'text': "What's in this image?"}, 
#       {'type': 'image_url', 'image_url': {'url': "data:image/png;base64,'/9j/4AAQSk'"}}
#     ]
#   },
#   {'role': 'assistant', 'content': 'I can see a beautiful landscape in the image.'}
# ]
```

#### å·¥å…·è°ƒç”¨è½¬æ¢ç¤ºä¾‹

```python
# ç¤ºä¾‹3: å·¥å…·è°ƒç”¨æ¶ˆæ¯è½¬æ¢
tool_messages = [
    SystemMessage("You are a calculator assistant."),
    HumanMessage("Calculate 15 + 27"),
    AIMessage(
        "", 
        tool_calls=[{
            "name": "add_numbers", 
            "args": {"a": 15, "b": 27}, 
            "id": "call_123", 
            "type": "tool_call"
        }]
    ),
    ToolMessage("42", tool_call_id="call_123", name="add_numbers"),
    AIMessage("The result of 15 + 27 is 42."),
]

openai_messages = convert_to_openai_messages(tool_messages)
print(openai_messages)
# è¾“å‡º:
# [
#   {'role': 'system', 'content': 'You are a calculator assistant.'},
#   {'role': 'user', 'content': 'Calculate 15 + 27'},
#   {
#     'role': 'assistant', 
#     'tool_calls': [{
#       'type': 'function', 
#       'id': 'call_123',
#       'function': {'name': 'add_numbers', 'arguments': '{"a": 15, "b": 27}'}
#     }], 
#     'content': ''
#   },
#   {'role': 'tool', 'name': 'add_numbers', 'content': '42'},
#   {'role': 'assistant', 'content': 'The result of 15 + 27 is 42.'}
# ]
```

#### æ–‡æœ¬æ ¼å¼å‚æ•°

`convert_to_openai_messages` å‡½æ•°æ”¯æŒ `text_format` å‚æ•°æ¥æ§åˆ¶æ–‡æœ¬å†…å®¹çš„æ ¼å¼åŒ–æ–¹å¼ï¼š

```python
# text_format="string" (é»˜è®¤): å°½å¯èƒ½ä¿æŒå­—ç¬¦ä¸²æ ¼å¼
messages_string = convert_to_openai_messages(messages, text_format="string")

# text_format="block": å°†æ‰€æœ‰æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºå—æ ¼å¼
messages_block = convert_to_openai_messages(messages, text_format="block")
```

#### å®é™…åº”ç”¨åœºæ™¯

**APIé›†æˆåœºæ™¯**:
```python
def send_to_openai_compatible_api(messages):
    """å°†LangChainæ¶ˆæ¯å‘é€åˆ°OpenAIå…¼å®¹çš„API"""
    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    openai_format = convert_to_openai_messages(messages)
    
    # å‘é€åˆ°ç¬¬ä¸‰æ–¹API
    response = requests.post(
        "https://api.example.com/v1/chat/completions",
        json={
            "model": "gpt-4o-mini",
            "messages": openai_format,
            "temperature": 0.7
        },
        headers={"Authorization": f"Bearer {api_key}"}
    )
    
    return response.json()
```

**æ¶ˆæ¯æ—¥å¿—è®°å½•åœºæ™¯**:
```python
def log_conversation(messages):
    """è®°å½•å¯¹è¯æ—¥å¿—ä¸ºOpenAIæ ¼å¼ä¾¿äºåˆ†æ"""
    openai_format = convert_to_openai_messages(messages)
    
    # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
    with open("conversation_log.json", "a") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "messages": openai_format
        }, f)
```

#### æ”¯æŒçš„æ¶ˆæ¯ç±»å‹è½¬æ¢å¯¹ç…§è¡¨

| LangChainæ¶ˆæ¯ç±»å‹ | OpenAIæ ¼å¼role | è¯´æ˜ |
|------------------|----------------|------|
| `SystemMessage` | `system` | ç³»ç»ŸæŒ‡ä»¤æ¶ˆæ¯ |
| `HumanMessage` | `user` | ç”¨æˆ·è¾“å…¥æ¶ˆæ¯ |
| `AIMessage` | `assistant` | AIå›å¤æ¶ˆæ¯ |
| `ToolMessage` | `tool` | å·¥å…·æ‰§è¡Œç»“æœæ¶ˆæ¯ |
| `FunctionMessage` | `function` | å‡½æ•°è°ƒç”¨ç»“æœæ¶ˆæ¯ï¼ˆå·²åºŸå¼ƒï¼‰ |

## ğŸ“Š å“åº”å…ƒæ•°æ® (Response Metadata)

### åŠŸèƒ½è¯´æ˜

è®¸å¤šæ¨¡å‹æä¾›å•†åœ¨å…¶èŠå¤©ç”Ÿæˆå“åº”ä¸­åŒ…å«ä¸€äº›å…ƒæ•°æ®ä¿¡æ¯ã€‚è¿™äº›å…ƒæ•°æ®å¯ä»¥é€šè¿‡ `AIMessage.response_metadata: Dict` å±æ€§è®¿é—®ã€‚æ ¹æ®æ¨¡å‹æä¾›å•†å’Œæ¨¡å‹é…ç½®çš„ä¸åŒï¼Œè¿™å¯èƒ½åŒ…å«ä»¤ç‰Œè®¡æ•°ã€æ—¥å¿—æ¦‚ç‡ç­‰ä¿¡æ¯ã€‚

### å®é™…åº”ç”¨åœºæ™¯

- **ğŸ“ˆ æˆæœ¬ç›‘æ§**: è·Ÿè¸ªtokenä½¿ç”¨é‡ï¼Œè®¡ç®—APIè°ƒç”¨æˆæœ¬
- **ğŸ” æ€§èƒ½åˆ†æ**: ç›‘æ§å“åº”æ—¶é—´å’Œå®ŒæˆçŠ¶æ€
- **ğŸ“‹ æ—¥å¿—è®°å½•**: è®°å½•è¯¦ç»†çš„è°ƒç”¨ä¿¡æ¯ç”¨äºè°ƒè¯•å’Œå®¡è®¡
- **âš¡ ä¼˜åŒ–å†³ç­–**: åŸºäºå…ƒæ•°æ®ä¼˜åŒ–æç¤ºè¯å’Œå‚æ•°è®¾ç½®
- **ğŸš¨ é”™è¯¯è¯Šæ–­**: é€šè¿‡finish_reasonç­‰ä¿¡æ¯è¯Šæ–­é—®é¢˜

### ä¸åŒæä¾›å•†çš„å“åº”å…ƒæ•°æ®ç¤ºä¾‹

#### OpenAI å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'token_usage': {
        'completion_tokens': 110,
        'prompt_tokens': 16,
        'total_tokens': 126,
        'completion_tokens_details': {
            'accepted_prediction_tokens': 0,
            'audio_tokens': 0,
            'reasoning_tokens': 0,
            'rejected_prediction_tokens': 0
        },
        'prompt_tokens_details': {
            'audio_tokens': 0, 
            'cached_tokens': 0
        }
    },
    'model_name': 'gpt-4o-mini-2024-07-18',
    'system_fingerprint': 'fp_b8bc95a0ac',
    'id': 'chatcmpl-BDrISvLar6AqcZngBmhajFZXVc2u9',
    'finish_reason': 'stop',
    'logprobs': None
}
```

#### Anthropic å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-sonnet-latest")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'id': 'msg_01JHnvPqgERY7MZwrvfkmq52',
    'model': 'claude-3-5-sonnet-20241022',
    'stop_reason': 'end_turn',
    'stop_sequence': None,
    'usage': {
        'cache_creation_input_tokens': 0,
        'cache_read_input_tokens': 0,
        'input_tokens': 17,
        'output_tokens': 221
    },
    'model_name': 'claude-3-5-sonnet-20241022'
}
```

#### Google VertexAI å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_google_vertexai import ChatVertexAI

llm = ChatVertexAI(model="gemini-2.0-flash-001")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'is_blocked': False,
    'safety_ratings': [],
    'usage_metadata': {
        'prompt_token_count': 10,
        'candidates_token_count': 55,
        'total_token_count': 65,
        'prompt_tokens_details': [{'modality': 1, 'token_count': 10}],
        'candidates_tokens_details': [{'modality': 1, 'token_count': 55}],
        'cached_content_token_count': 0,
        'cache_tokens_details': []
    },
    'finish_reason': 'STOP',
    'avg_logprobs': -0.251378042047674,
    'model_name': 'gemini-2.0-flash-001'
}
```

#### AWS Bedrock (Anthropic) å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(model="anthropic.claude-3-sonnet-20240229-v1:0")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'ResponseMetadata': {
        'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',
        'HTTPStatusCode': 200,
        'HTTPHeaders': {
            'date': 'Sat, 22 Mar 2025 11:27:46 GMT',
            'content-type': 'application/json',
            'content-length': '1660',
            'connection': 'keep-alive',
            'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'
        },
        'RetryAttempts': 0
    },
    'stopReason': 'end_turn',
    'metrics': {'latencyMs': [11044]}
}
```

#### MistralAI å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(model="mistral-small-latest")
msg = llm.invoke([("human", "What's the oldest known example of cuneiform")])
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'token_usage': {
        'prompt_tokens': 13,
        'total_tokens': 219,
        'completion_tokens': 206
    },
    'model_name': 'mistral-small-latest',
    'model': 'mistral-small-latest',
    'finish_reason': 'stop'
}
```

#### Groq å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'token_usage': {
        'completion_tokens': 184,
        'prompt_tokens': 45,
        'total_tokens': 229,
        'completion_time': 0.245333333,
        'prompt_time': 0.002262803,
        'queue_time': 0.19315161,
        'total_time': 0.247596136
    },
    'model_name': 'llama-3.1-8b-instant',
    'system_fingerprint': 'fp_a56f6eea01',
    'finish_reason': 'stop',
    'logprobs': None
}
```

#### FireworksAI å…ƒæ•°æ®ç¤ºä¾‹

```python
from langchain_fireworks import ChatFireworks

llm = ChatFireworks(model="accounts/fireworks/models/llama-v3p1-70b-instruct")
msg = llm.invoke("What's the oldest known example of cuneiform")
print(msg.response_metadata)

# è¾“å‡ºç¤ºä¾‹:
{
    'token_usage': {
        'prompt_tokens': 25,
        'total_tokens': 352,
        'completion_tokens': 327
    },
    'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct',
    'system_fingerprint': '',
    'finish_reason': 'stop',
    'logprobs': None
}
```

### å…ƒæ•°æ®å­—æ®µè¯´æ˜

#### é€šç”¨å­—æ®µ

| å­—æ®µå | è¯´æ˜ | ç”¨é€” |
|--------|------|------|
| `token_usage` | Tokenä½¿ç”¨ç»Ÿè®¡ | æˆæœ¬è®¡ç®—å’Œä¼˜åŒ– |
| `model_name` | å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§° | ç‰ˆæœ¬è·Ÿè¸ª |
| `finish_reason` | å®ŒæˆåŸå›  | åˆ¤æ–­å“åº”æ˜¯å¦å®Œæ•´ |
| `id` | è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦ | æ—¥å¿—è¿½è¸ªå’Œè°ƒè¯• |

#### Tokenä½¿ç”¨å­—æ®µ

| å­—æ®µå | è¯´æ˜ | é‡è¦æ€§ |
|--------|------|--------|
| `prompt_tokens` | è¾“å…¥tokenæ•°é‡ | ğŸ’° æˆæœ¬è®¡ç®— |
| `completion_tokens` | è¾“å‡ºtokenæ•°é‡ | ğŸ’° æˆæœ¬è®¡ç®— |
| `total_tokens` | æ€»tokenæ•°é‡ | ğŸ“Š ä½¿ç”¨é‡ç›‘æ§ |
| `cached_tokens` | ç¼“å­˜tokenæ•°é‡ | âš¡ æ€§èƒ½ä¼˜åŒ– |

#### å®ŒæˆçŠ¶æ€å­—æ®µ

| finish_reason | è¯´æ˜ | å¤„ç†å»ºè®® |
|---------------|------|----------|
| `stop` | æ­£å¸¸å®Œæˆ | âœ… æ— éœ€å¤„ç† |
| `length` | è¾¾åˆ°æœ€å¤§é•¿åº¦é™åˆ¶ | âš ï¸ è€ƒè™‘å¢åŠ max_tokens |
| `content_filter` | å†…å®¹è¢«è¿‡æ»¤ | ğŸš« è°ƒæ•´è¾“å…¥å†…å®¹ |
| `tool_calls` | éœ€è¦å·¥å…·è°ƒç”¨ | ğŸ”§ æ‰§è¡Œå·¥å…·è°ƒç”¨ |

### å®é™…ä½¿ç”¨ç¤ºä¾‹

#### æˆæœ¬ç›‘æ§ç¤ºä¾‹

```python
def monitor_api_costs(messages: list) -> dict:
    """ç›‘æ§APIè°ƒç”¨æˆæœ¬"""
    response = llm.invoke(messages)
    
    # è·å–tokenä½¿ç”¨æƒ…å†µ
    metadata = response.response_metadata
    token_usage = metadata.get('token_usage', {})
    
    # è®¡ç®—æˆæœ¬ï¼ˆä»¥OpenAIä¸ºä¾‹ï¼‰
    prompt_cost = token_usage.get('prompt_tokens', 0) * 0.00015 / 1000
    completion_cost = token_usage.get('completion_tokens', 0) * 0.0006 / 1000
    total_cost = prompt_cost + completion_cost
    
    return {
        'response': response.content,
        'cost_info': {
            'prompt_tokens': token_usage.get('prompt_tokens', 0),
            'completion_tokens': token_usage.get('completion_tokens', 0),
            'total_tokens': token_usage.get('total_tokens', 0),
            'estimated_cost_usd': total_cost
        },
        'model_info': {
            'model': metadata.get('model_name'),
            'finish_reason': metadata.get('finish_reason')
        }
    }

# ä½¿ç”¨ç¤ºä¾‹
result = monitor_api_costs([HumanMessage("Explain quantum computing")])
print(f"Cost: ${result['cost_info']['estimated_cost_usd']:.6f}")
print(f"Tokens used: {result['cost_info']['total_tokens']}")
```

#### æ€§èƒ½åˆ†æç¤ºä¾‹

```python
import time
from datetime import datetime

def analyze_performance(messages: list) -> dict:
    """åˆ†æAPIè°ƒç”¨æ€§èƒ½"""
    start_time = time.time()
    timestamp = datetime.now()
    
    response = llm.invoke(messages)
    
    end_time = time.time()
    response_time = end_time - start_time
    
    metadata = response.response_metadata
    
    return {
        'timestamp': timestamp.isoformat(),
        'response_time_seconds': response_time,
        'token_info': metadata.get('token_usage', {}),
        'model_info': metadata.get('model_name'),
        'finish_reason': metadata.get('finish_reason'),
        'request_id': metadata.get('id'),
        'tokens_per_second': metadata.get('token_usage', {}).get('completion_tokens', 0) / response_time if response_time > 0 else 0
    }

# ä½¿ç”¨ç¤ºä¾‹
perf_data = analyze_performance([HumanMessage("Write a short story")])
print(f"Response time: {perf_data['response_time_seconds']:.2f}s")
print(f"Tokens/second: {perf_data['tokens_per_second']:.1f}")
```

#### é”™è¯¯è¯Šæ–­ç¤ºä¾‹

```python
def diagnose_response(response) -> dict:
    """è¯Šæ–­å“åº”çŠ¶æ€"""
    metadata = response.response_metadata
    finish_reason = metadata.get('finish_reason')
    
    diagnosis = {
        'status': 'unknown',
        'message': '',
        'suggestions': []
    }
    
    if finish_reason == 'stop':
        diagnosis.update({
            'status': 'success',
            'message': 'å“åº”æ­£å¸¸å®Œæˆ'
        })
    elif finish_reason == 'length':
        diagnosis.update({
            'status': 'warning',
            'message': 'å“åº”å› é•¿åº¦é™åˆ¶è¢«æˆªæ–­',
            'suggestions': [
                'å¢åŠ max_tokenså‚æ•°',
                'æ‹†åˆ†æˆå¤šä¸ªè¾ƒçŸ­çš„è¯·æ±‚',
                'ä¼˜åŒ–æç¤ºè¯ä»¥è·å¾—æ›´ç®€æ´çš„å›ç­”'
            ]
        })
    elif finish_reason == 'content_filter':
        diagnosis.update({
            'status': 'error',
            'message': 'å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆª',
            'suggestions': [
                'ä¿®æ”¹è¾“å…¥å†…å®¹é¿å…æ•æ„Ÿè¯é¢˜',
                'ä½¿ç”¨æ›´ä¸­æ€§çš„è¡¨è¾¾æ–¹å¼',
                'æ£€æŸ¥å†…å®¹æ˜¯å¦ç¬¦åˆä½¿ç”¨æ”¿ç­–'
            ]
        })
    
    return diagnosis

# ä½¿ç”¨ç¤ºä¾‹
response = llm.invoke([HumanMessage("Your question here")])
diagnosis = diagnose_response(response)
print(f"Status: {diagnosis['status']}")
print(f"Message: {diagnosis['message']}")
if diagnosis['suggestions']:
    print("Suggestions:")
    for suggestion in diagnosis['suggestions']:
        print(f"  - {suggestion}")
```

### æµ‹è¯•ä¸­çš„å…ƒæ•°æ®éªŒè¯

åœ¨æµ‹è¯•å¥—ä»¶ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯å“åº”å…ƒæ•°æ®çš„æ­£ç¡®æ€§ï¼š

```python
def test_response_metadata():
    """æµ‹è¯•å“åº”å…ƒæ•°æ®çš„å®Œæ•´æ€§"""
    response = llm.invoke([HumanMessage("Hello")])
    
    # éªŒè¯å…ƒæ•°æ®å­˜åœ¨
    assert hasattr(response, 'response_metadata')
    assert isinstance(response.response_metadata, dict)
    
    # éªŒè¯å¿…è¦å­—æ®µ
    metadata = response.response_metadata
    assert 'model_name' in metadata
    assert 'finish_reason' in metadata
    
    # éªŒè¯tokenä½¿ç”¨ä¿¡æ¯ï¼ˆå¦‚æœæä¾›å•†æ”¯æŒï¼‰
    if 'token_usage' in metadata:
        token_usage = metadata['token_usage']
        assert 'total_tokens' in token_usage
        assert token_usage['total_tokens'] > 0
        
    # éªŒè¯å®ŒæˆçŠ¶æ€
    assert metadata['finish_reason'] in ['stop', 'length', 'content_filter', 'tool_calls']
    
    print(f"âœ… å…ƒæ•°æ®éªŒè¯é€šè¿‡: {metadata['model_name']}")
    print(f"   å®ŒæˆçŠ¶æ€: {metadata['finish_reason']}")
    if 'token_usage' in metadata:
        print(f"   Tokenä½¿ç”¨: {metadata['token_usage']['total_tokens']}")
```