"""
å¼‚æ­¥æ“ä½œ vs åŒæ­¥æ“ä½œå®é™…åº”ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•åœ¨çœŸå®åœºæ™¯ä¸­ä½¿ç”¨å¼‚æ­¥æ“ä½œæé«˜æ€§èƒ½
"""

import asyncio
import time
from typing import List, Dict
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))

from src.config.api import apis


def get_chat_model() -> ChatOpenAI:
    """
    åˆ›å»ºæ™®é€šèŠå¤©æ¨¡å‹å®ä¾‹
    
    Returns:
        ChatOpenAI: èŠå¤©æ¨¡å‹å®ä¾‹
    """
    config = apis["local"]
    return ChatOpenAI(
        model="gpt-4o-mini",
        base_url=config["base_url"],
        api_key=config["api_key"],
        temperature=0.7,
        max_tokens=200
    )


class CustomerServiceBot:
    """å®¢æœæœºå™¨äººç¤ºä¾‹ - å±•ç¤ºå¼‚æ­¥æ“ä½œçš„å®é™…åº”ç”¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®¢æœæœºå™¨äºº"""
        self.model = get_chat_model()
        self.system_message = SystemMessage(
            content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        )
    
    def handle_single_request_sync(self, user_question: str) -> Dict[str, str]:
        """
        åŒæ­¥å¤„ç†å•ä¸ªç”¨æˆ·è¯·æ±‚
        
        Args:
            user_question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Dict[str, str]: å¤„ç†ç»“æœ
        """
        messages = [self.system_message, HumanMessage(content=user_question)]
        
        start_time = time.time()
        response = self.model.invoke(messages)
        end_time = time.time()
        
        return {
            "question": user_question,
            "answer": response.content,
            "duration": f"{end_time - start_time:.2f}s"
        }
    
    async def handle_single_request_async(self, user_question: str) -> Dict[str, str]:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªç”¨æˆ·è¯·æ±‚
        
        Args:
            user_question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Dict[str, str]: å¤„ç†ç»“æœ
        """
        messages = [self.system_message, HumanMessage(content=user_question)]
        
        start_time = time.time()
        response = await self.model.ainvoke(messages)
        end_time = time.time()
        
        return {
            "question": user_question,
            "answer": response.content,
            "duration": f"{end_time - start_time:.2f}s"
        }
    
    def handle_multiple_requests_sync(self, questions: List[str]) -> List[Dict[str, str]]:
        """
        åŒæ­¥å¤„ç†å¤šä¸ªç”¨æˆ·è¯·æ±‚ï¼ˆé¡ºåºæ‰§è¡Œï¼‰
        
        Args:
            questions: ç”¨æˆ·é—®é¢˜åˆ—è¡¨
            
        Returns:
            List[Dict[str, str]]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        print("ğŸ”„ åŒæ­¥å¤„ç†å¤šä¸ªè¯·æ±‚ï¼ˆé¡ºåºæ‰§è¡Œï¼‰...")
        start_time = time.time()
        
        results = []
        for question in questions:
            result = self.handle_single_request_sync(question)
            results.append(result)
            print(f"âœ… å®Œæˆ: {question[:30]}... ({result['duration']})")
        
        total_time = time.time() - start_time
        print(f"â±ï¸ åŒæ­¥æ€»è€—æ—¶: {total_time:.2f}s")
        return results
    
    async def handle_multiple_requests_async(self, questions: List[str]) -> List[Dict[str, str]]:
        """
        å¼‚æ­¥å¤„ç†å¤šä¸ªç”¨æˆ·è¯·æ±‚ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
        
        Args:
            questions: ç”¨æˆ·é—®é¢˜åˆ—è¡¨
            
        Returns:
            List[Dict[str, str]]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        print("âš¡ å¼‚æ­¥å¤„ç†å¤šä¸ªè¯·æ±‚ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰...")
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [self.handle_single_request_async(question) for question in questions]
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)
        
        total_time = time.time() - start_time
        print(f"âš¡ å¼‚æ­¥æ€»è€—æ—¶: {total_time:.2f}s")
        
        for result in results:
            print(f"âœ… å®Œæˆ: {result['question'][:30]}... ({result['duration']})")
        
        return results


class StreamingChatExample:
    """æµå¼èŠå¤©ç¤ºä¾‹ - å±•ç¤ºå®æ—¶å“åº”"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµå¼èŠå¤©"""
        config = apis["local"]
        self.model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            temperature=0.7,
            streaming=True
        )
    
    async def streaming_chat(self, user_input: str) -> None:
        """
        å¼‚æ­¥æµå¼èŠå¤©
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
        """
        messages = [HumanMessage(content=user_input)]
        
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")
        print("ğŸ¤– AI: ", end="", flush=True)
        
        # å¼‚æ­¥æµå¼è¾“å‡º
        async for chunk in self.model.astream(messages):
            if chunk.content:
                print(chunk.content, end="", flush=True)
        
        print("\n" + "="*50)


async def real_world_example():
    """çœŸå®ä¸–ç•Œåº”ç”¨ç¤ºä¾‹"""
    print("ğŸŒŸ çœŸå®ä¸–ç•Œå¼‚æ­¥åº”ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # æ¨¡æ‹Ÿå®¢æœåœºæ™¯
    customer_service = CustomerServiceBot()
    
    # æ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·åŒæ—¶æé—®
    user_questions = [
        "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ",
        "å¦‚ä½•é€€æ¢è´§ï¼Ÿ",
        "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ",
        "äº§å“è´¨é‡æ€ä¹ˆæ ·ï¼Ÿ",
        "æ”¯ä»˜æ–¹å¼æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    print("ğŸ“‹ å¤„ç†5ä¸ªç”¨æˆ·é—®é¢˜...")
    print()
    
    # 1. åŒæ­¥å¤„ç†ï¼ˆé¡ºåºæ‰§è¡Œï¼‰
    sync_results = customer_service.handle_multiple_requests_sync(user_questions)
    print()
    
    # 2. å¼‚æ­¥å¤„ç†ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
    async_results = await customer_service.handle_multiple_requests_async(user_questions)
    print()
    
    # 3. æµå¼èŠå¤©æ¼”ç¤º
    print("ğŸ’¬ æµå¼èŠå¤©æ¼”ç¤º:")
    streaming_chat = StreamingChatExample()
    await streaming_chat.streaming_chat("è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²")


async def concurrent_model_usage_example():
    """åŒä¸€ä¸ªæ¨¡å‹å¤„ç†å¹¶å‘è¯·æ±‚çš„ç¤ºä¾‹"""
    print("ğŸ”€ åŒä¸€ä¸ªæ¨¡å‹å¤„ç†å¹¶å‘è¯·æ±‚ç¤ºä¾‹")
    print("="*60)
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡å‹å®ä¾‹
    model = get_chat_model()
    
    # æ¨¡æ‹Ÿ5ä¸ªä¸åŒç”¨æˆ·çš„é—®é¢˜
    user_requests = [
        ("ç”¨æˆ·A", "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"),
        ("ç”¨æˆ·B", "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"),
        ("ç”¨æˆ·C", "äººå·¥æ™ºèƒ½æœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ"),
        ("ç”¨æˆ·D", "æ•°æ®ç§‘å­¦éœ€è¦ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ"),
        ("ç”¨æˆ·E", "äº‘è®¡ç®—çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ")
    ]
    
    async def process_user_request(user_id: str, question: str) -> Dict[str, str]:
        """
        å¤„ç†å•ä¸ªç”¨æˆ·è¯·æ±‚
        
        Args:
            user_id: ç”¨æˆ·ID
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Dict[str, str]: å¤„ç†ç»“æœ
        """
        print(f"ğŸ“¨ {user_id} å‘èµ·è¯·æ±‚: {question}")
        messages = [HumanMessage(content=question)]
        
        start_time = time.time()
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹å¤„ç†ä¸åŒç”¨æˆ·çš„è¯·æ±‚
        response = await model.ainvoke(messages)
        duration = time.time() - start_time
        
        result = {
            "user": user_id,
            "question": question,
            "answer": response.content[:100] + "..." if len(response.content) > 100 else response.content,
            "duration": f"{duration:.2f}s"
        }
        
        print(f"âœ… {user_id} è¯·æ±‚å®Œæˆ ({result['duration']})")
        return result
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰ç”¨æˆ·è¯·æ±‚
    print("âš¡ å¼€å§‹å¹¶å‘å¤„ç†...")
    start_time = time.time()
    
    tasks = [process_user_request(user_id, question) for user_id, question in user_requests]
    results = await asyncio.gather(*tasks)
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ æ‰€æœ‰è¯·æ±‚å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}s")
    
    print("\nğŸ“Š å¤„ç†ç»“æœ:")
    for result in results:
        print(f"  {result['user']}: {result['answer']}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼‚æ­¥æ“ä½œå®é™…åº”ç”¨æ¼”ç¤º")
    print("="*80)
    
    # 1. çœŸå®ä¸–ç•Œåº”ç”¨ç¤ºä¾‹
    await real_world_example()
    print("\n")
    
    # 2. åŒä¸€ä¸ªæ¨¡å‹å¹¶å‘ä½¿ç”¨ç¤ºä¾‹
    await concurrent_model_usage_example()
    
    print("\nğŸ“ æ€»ç»“:")
    print("1. å¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨å•çº¿ç¨‹ + åç¨‹ï¼Œä¸æ˜¯å¤šçº¿ç¨‹")
    print("2. åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹å¯ä»¥å®‰å…¨åœ°å¤„ç†å¹¶å‘è¯·æ±‚")
    print("3. å¼‚æ­¥æ“ä½œåœ¨IOå¯†é›†å‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½")
    print("4. é€‚ç”¨åœºæ™¯ï¼šWebæœåŠ¡ã€èŠå¤©æœºå™¨äººã€æ‰¹é‡å¤„ç†ç­‰")


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main()) 