"""
æµå¼è¾“å‡ºæµ‹è¯•

æµ‹è¯•ChatOpenAIæ¨¡å‹çš„æµå¼è¾“å‡ºåŠŸèƒ½
"""

import unittest
from typing import List, Iterator

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.outputs import ChatGenerationChunk
from langchain_core.callbacks import StreamingStdOutCallbackHandler

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../src'))

from src.config.api import apis


class TestStreamingChat(unittest.TestCase):
    """æµå¼èŠå¤©æµ‹è¯•ç±»"""
    
    def get_streaming_chat_model(self) -> ChatOpenAI:
        """
        åˆ›å»ºæ”¯æŒæµå¼è¾“å‡ºçš„ChatOpenAIå®ä¾‹
        
        Returns:
            ChatOpenAI: é…ç½®å¥½çš„æµå¼èŠå¤©æ¨¡å‹å®ä¾‹
        """
        config = apis["local"]
        return ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            temperature=0.7,
            max_tokens=1000,
            streaming=True,  # å¯ç”¨æµå¼è¾“å‡º
            callbacks=[StreamingStdOutCallbackHandler()]
        )
    
    def test_basic_streaming(self) -> None:
        """
        æµ‹è¯•åŸºæœ¬æµå¼è¾“å‡ºåŠŸèƒ½
        """
        streaming_model = self.get_streaming_chat_model()
        messages = [HumanMessage(content="Count from 1 to 3")]
        
        try:
            # æ”¶é›†æ‰€æœ‰chunks
            response_chunks = list(streaming_model.stream(messages))
            
            self.assertGreater(len(response_chunks), 0)
            
            # éªŒè¯å†…å®¹
            full_response = "".join([chunk.content for chunk in response_chunks])
            print(f"Basic streaming response: {full_response}")
            
            # æ‰“å°æ¯ä¸ªchunk
            for i, chunk in enumerate(response_chunks):
                print(f"Chunk {i}: {chunk.content}")
                
        except Exception as e:
            print(f"Basic streaming test failed: {e}")
    
    def test_streaming_with_system_message(self) -> None:
        """
        æµ‹è¯•åŒ…å«ç³»ç»Ÿæ¶ˆæ¯çš„æµå¼è¾“å‡º
        """
        streaming_model = self.get_streaming_chat_model()
        messages = [
            SystemMessage(content="You are a helpful assistant that counts slowly."),
            HumanMessage(content="Count from 1 to 2")
        ]
        
        try:
            response_chunks = list(streaming_model.stream(messages))
            
            self.assertGreater(len(response_chunks), 1)
            full_response = "".join([chunk.content for chunk in response_chunks])
            print(f"System message streaming response: {full_response}")
        except Exception as e:
            print(f"System message streaming test failed: {e}")
    
    def test_streaming_callback_handler(self) -> None:
        """
        æµ‹è¯•æµå¼è¾“å‡ºå›è°ƒå¤„ç†å™¨
        """
        # åˆ›å»ºè‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨
        callback_outputs = []
        
        class TestCallbackHandler(StreamingStdOutCallbackHandler):
            def on_llm_new_token(self, token: str, **kwargs) -> None:
                callback_outputs.append(token)
                print(f"Callback received token: {token}")
        
        config = apis["local"]
        streaming_model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            streaming=True,
            callbacks=[TestCallbackHandler()]
        )
        
        # éªŒè¯æ¨¡å‹é…ç½®äº†å›è°ƒ
        self.assertIsNotNone(streaming_model.callbacks)
        self.assertEqual(len(streaming_model.callbacks), 1)
        self.assertIsInstance(streaming_model.callbacks[0], TestCallbackHandler)
        
        messages = [HumanMessage(content="Say hello")]
        
        try:
            response_chunks = list(streaming_model.stream(messages))
            print(f"Callback handler collected {len(callback_outputs)} tokens")
        except Exception as e:
            print(f"Callback handler test failed: {e}")
    
    def test_streaming_chunk_format(self) -> None:
        """
        æµ‹è¯•æµå¼è¾“å‡ºchunkçš„æ ¼å¼
        """
        streaming_model = self.get_streaming_chat_model()
        messages = [HumanMessage(content="Say hello")]
        
        try:
            for chunk in streaming_model.stream(messages):
                # éªŒè¯chunkæ ¼å¼
                self.assertIsInstance(chunk, ChatGenerationChunk)
                self.assertTrue(hasattr(chunk, 'content'))
                self.assertIsInstance(chunk.content, str)
                print(f"Chunk format - Type: {type(chunk)}, Content: '{chunk.content}'")
        except Exception as e:
            print(f"Chunk format test failed: {e}")
    
    def test_streaming_long_response(self) -> None:
        """
        æµ‹è¯•é•¿å“åº”çš„æµå¼å¤„ç†
        """
        streaming_model = self.get_streaming_chat_model()
        messages = [HumanMessage(content="Write a short story about a cat (about 100 words)")]
        
        try:
            response_chunks = list(streaming_model.stream(messages))
            
            self.assertGreater(len(response_chunks), 10)  # åº”è¯¥æœ‰å¤šä¸ªchunks
            full_response = "".join([chunk.content for chunk in response_chunks])
            print(f"Long response ({len(response_chunks)} chunks): {full_response}")
            print(f"Total response length: {len(full_response)} characters")
        except Exception as e:
            print(f"Long response streaming test failed: {e}")
    
    def test_streaming_model_configuration(self) -> None:
        """
        æµ‹è¯•æµå¼æ¨¡å‹é…ç½®
        """
        config = apis["local"]
        
        # æµ‹è¯•å¯ç”¨æµå¼è¾“å‡º
        streaming_model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            streaming=True
        )
        self.assertTrue(streaming_model.streaming)
        
        # æµ‹è¯•ç¦ç”¨æµå¼è¾“å‡º
        non_streaming_model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            streaming=False
        )
        self.assertFalse(non_streaming_model.streaming)
        
        print("Streaming configuration test passed")
    
    def test_streaming_vs_normal(self) -> None:
        """
        æ¯”è¾ƒæµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º
        """
        config = apis["local"]
        
        # æµå¼æ¨¡å‹
        streaming_model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            streaming=True
        )
        
        # æ™®é€šæ¨¡å‹
        normal_model = ChatOpenAI(
            model="gpt-4o-mini",
            base_url=config["base_url"],
            api_key=config["api_key"],
            streaming=False
        )
        
        messages = [HumanMessage(content="What is 2+2?")]
        
        try:
            # æµå¼è¾“å‡º
            streaming_chunks = list(streaming_model.stream(messages))
            streaming_response = "".join([chunk.content for chunk in streaming_chunks])
            
            # æ™®é€šè¾“å‡º
            normal_response = normal_model.invoke(messages)
            
            print(f"Streaming response ({len(streaming_chunks)} chunks): {streaming_response}")
            print(f"Normal response: {normal_response.content}")
            
        except Exception as e:
            print(f"Streaming vs normal test failed: {e}")


def main() -> int:
    """
    è¿è¡Œæµå¼è¾“å‡ºæµ‹è¯•çš„ä¸»å‡½æ•°
    
    Returns:
        int: é€€å‡ºç ï¼Œ0è¡¨ç¤ºæˆåŠŸ
    """
    print("ğŸš€ è¿è¡Œæµå¼è¾“å‡ºæµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    unittest.main(verbosity=2)
    return 0


if __name__ == "__main__":
    main()